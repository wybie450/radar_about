```
export QT_ENABLE_HIGHDPI_SCALING=0
killall komorebi
#查看tf树
ros2 run tf2_tools view_frames
#打开foxglove桥接器
ros2 launch foxglove_bridge foxglove_bridge_launch.xml
rqt-tools
#查看点云字段
ros2 topic echo --once 点云话题 | grep -A20 "fields"
#查询pytorch版本
python -c "import torch; print(torch.__version__)"
#保留多少行代码
 head -n 151 val.txt > val.txt.new && mv val.txt.new val.txt
 #查看pcd
pcl_viewer your_file.pcd
```

# ==雷达原理==

功能方面https://zhuanlan.zhihu.com/p/675926305

激光雷达可根据其不同的成像结构和测量原理进行分类，激光雷达的成像机制可分为三大类：机械激光雷达、[扫描固态激光雷达](https://zhida.zhihu.com/search?content_id=238335906&content_type=Article&match_order=1&q=扫描固态激光雷达&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjQwNTYzNjAsInEiOiLmiavmj4_lm7rmgIHmv4DlhYnpm7fovr4iLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzgzMzU5MDYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.6fyd64-JTWvESHL1iXliKNejEJ0gObg7yoUXGbijel4&zhida_source=entity)和非扫描结构的激光雷达。关于测量原理，主要类型包括[脉冲飞行时间](https://zhida.zhihu.com/search?content_id=238335906&content_type=Article&match_order=1&q=脉冲飞行时间&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjQwNTYzNjAsInEiOiLohInlhrLpo57ooYzml7bpl7QiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzgzMzU5MDYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.on0wNwvmU4OEkTlPTU3xDMlSJQEdWLKGh0hseT5TehQ&zhida_source=entity)（ToF）、调幅连续波（AMCW）和[调频连续波](https://zhida.zhihu.com/search?content_id=238335906&content_type=Article&match_order=1&q=调频连续波&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NjQwNTYzNjAsInEiOiLosIPpopHov57nu63ms6IiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzgzMzU5MDYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.CfMRt96ZW5EuU-UnlROGQ6DDIcBeHZB7szFDKQbMfP0&zhida_source=entity)（FMCW）激光雷达。

### 超参数

**1. 点云范围（Point Cloud Range / ROI）**

- 控制算法在 **XYZ 空间** 内处理的区域
- 用来过滤无效点、减少计算量

**调大：** 更广但更慢、噪声更多
 **调小：** 更快但可能裁掉目标
 **例子：** `x[-50,50], y[-50,50], z[-3,3]`

------

**2. 体素大小（Voxel Size）**

- 将点云划分成固定大小的“立方体网格”
- 用于下采样、加速、构建 pillar/grid

**调大：** 更快但精度下降
 **调小：** 更准但显存↑ 速度↓
 **例子：** `0.1m × 0.1m × 0.2m`

------

**3. 点云特征（Point Features）**

- 点的输入维度，如 `xyz/intensity/ring/timestamp`
- 决定模型可以使用哪些信息

**增加特征：** 精度↑ 计算↑
 **减少特征：** 更轻量但信息减少
 **例子：** `xyz + intensity`

------

**4. 最大点数限制（Max Points per Voxel）**

- 每个体素允许的点数上限
- 控制点云密度、防止 batch 爆炸

**调大：** 保留细节但慢
 **调小：** 更快但可能丢信息
 **例子：** `5`, `10`

------

**5. 下采样分辨率（Downsample Resolution）**

- 降低点云点数的尺度
- 常用于体素网格下采样（VoxelGrid）

**调大：** 快但粗糙
 **调小：** 准但慢
 **例子：** `0.1m` 体素

------

**6. 聚类距离阈值（Clustering eps / Radius）**

- 用于 DBSCAN / 欧式聚类
- 决定点簇的大小

**调大：** 聚类变大、目标可能合并
 **调小：** 分得更细、小目标更容易分开
 **例子：** `0.3m – 0.8m`

------

**7. 最小聚类点数（min_points）**

- 小于该点数的 cluster 被视为噪点

**调大：** 去噪更强，但丢小目标
 **调小：** 保留小物体，但噪点增多
 **例子：** `5 – 20`

------

**8. 最大有效距离（Max LiDAR Range）**

- 超过这个距离的点被视为噪声

**调大：** 范围大但噪音多
 **调小：** 稳定但看得近
 **例子：** `80m – 120m`

------

**9. 地面去除阈值（Ground Removal Threshold）**

- 判断地面点的高度差阈值

**调大：** 地面去除更多但可能误删低障碍
 **调小：** 更保守但地面残留多
 **例子：** `0.1m – 0.2m`

------

**10. 半径搜索（kNN Radius Search）**

- 用于 ICP / 法向量估计 / 特征提取

**调大：** 更平滑但更慢
 **调小：** 更快但不稳定
 **例子：** `0.3m – 1.0m`

------

**11. 滑动窗口大小（SLAM Window Size）**

- SLAM 使用的历史帧数量

**调大：** 更稳定但慢
 **调小：** 更快但可能抖动
 **例子：** `5 – 10`

## 点云图技术（3D-SLAM）

### PCL处理

**PCL（Point Cloud Library，点云库）** 是一个 **开源的、跨平台的 C++ 编程库**，专门用于 **2D/3D 点云数据的处理**。

它提供了大量高效、模块化的算法，支持：

- 点云的**读取 / 写入**
- **滤波（去噪、下采样）**
- **分割（地面提取、物体分离）**
- **特征提取（法线、曲率、关键点）**
- **配准（点云对齐，比如 ICP）**
- **表面重建**
- **聚类（比如物体分割）**
- **可视化**

### fast-lio

```
ros2 service call /map_save std_srvs/srv/Trigger
```

https://blog.csdn.net/lovely_yoshino/article/details/124907915

传统点云处理一般是对于特征点进行提取显现，在fastlio算法中是对于所有点进行一个优化显现。

1. 将IMU和Lidar特征点紧耦合在一起

2. 使用反向传播考虑到了运动补偿

3. 将IEKF的卡尔曼滤波 PHT(HPHT+R)−1转为 (HTR−1H+P−1)−1HTR−1，这样做的好处是原本公式中求逆中的矩阵是观测维度的，新公式求逆中的矩阵是状态维度的，需求逆H的矩阵维度减小了（因为在实际情况中，激光特征点的数量维度要远大于状态量的维度）

   顺序采样的激光雷达原始点首先在10ms（用于100Hz更新）和100ms（用于10Hz更新）之间的时间段内累积。

   ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/417a4a363ec27f7dcdae60daae839bf7.png)

FAST-LIO / point_lio 的点云前处理结构里通常维护三个点云集合：

| 名称        | 含义                            | 用途                                |
| ----------- | ------------------------------- | ----------------------------------- |
| **pl_corn** | corner features（角点）         | 用于几何优化中的角点约束            |
| **pl_surf** | surface features（平面点）      | 用于平面约束，提高稳定性            |
| **pl_full** | 原始点云 / 全点云（full cloud） | 用于发布 / 显示（cloud_registered） |

### point-lio

https://zhuanlan.zhihu.com/p/621770870

相同用fast-lio的idktree,但是                         相对通过算法对于极端运动下imu的数据误差进行了补偿，在建图下失真较少

### vslam算法

- **基本框架：**

  https://zhuanlan.zhihu.com/p/121601875

  视觉里程计，用tof模仿激光原理，建出彩色点云图

#### 二叉树

https://blog.csdn.net/qq_41404557/article/details/115447169

ikdtree：**一种有效的动态空间划分的数据结构**，ikd树只使用加入的数据点来增量的更新k-d树，比现有静态k-d树的计算时间少很多。也即在每个节点处，除了计算节点本身与查询点的距离之外，也会分别判断左右两个子树的range 是否与目标解空间有重叠，只有有重叠的子树才会被继续递归搜索

https://zhuanlan.zhihu.com/p/529926254

<img src="https://i-blog.csdnimg.cn/blog_migrate/8f95571559605bca995ce1e094b95698.png" alt="img" style="zoom: 33%;" />

## 滤波算法

### 高斯滤波

**高斯滤波是一种线性平滑滤波器，它使用高斯函数（正态分布）作为权重模板，对图像（或数据）中的每个像素（或点）进行加权平均，从而达到去噪、平滑的效果。**高斯滤波是一种基于高斯函数的图像或点云平滑技术，通过加权平均邻域数据来去除噪声，同时尽量保留边缘信息，是图像处理和计算机视觉中最常用的预处理方法之一。

### 卡尔曼滤波

**卡尔曼滤波（Kalman Filter, KF）是一种递归的、最优的、基于线性系统和高斯噪声假设的状态估计算法。**从不同角度殊途同归地得到了最优估计值和预测值、观测值的关系式，并且希望求解一个合适的K，使得最优估计值最接近真实值。通过观测值来修正预测值

有扩展卡尔曼等滤波方式，出现的原因是因为机器人由传感器计算代偿时考虑到的误差稳定，从而对其数据进行的优化。

<img src="https://i-blog.csdnimg.cn/blog_migrate/63235ce47d9e92d66b5630eb9f34add5.png" alt="img" style="zoom:67%;" />

#### 线性卡尔曼

基本滤波：https://blog.csdn.net/navsense_support/article/details/146210928

卡尔曼滤波：https://blog.csdn.net/navsense_support/article/details/146231513

（线性）卡尔曼滤波的应用基于以下三个假设前提：

1. 当前时刻状态只和**上一时刻**状态有关。
2. 模型和系统均满足**线性**关系。
3. 引入的噪声符合**高斯**分布。

**矩阵 H的作用**：**只让测量到的部分进行修正**，不会影响其他状态变量。

修正方程：

<img src="雷达.assets/image-20251023193221379.png" alt="image-20251023193221379" style="zoom:50%;" />

衡量两个随机变量 **X** 和 **Y** 之间共同变化趋势的度量,样本协方差的公式为：

![image-20251023174218212](雷达.assets/image-20251023174218212.png)

（先验）测量误差：

<img src="雷达.assets/image-20251023211547021.png" alt="image-20251023211547021" style="zoom: 67%;" />

这个是在运动过程中传感器自身产生的过程噪声，B为控制量矩阵，uk为控制量，wk为过程噪声

（后验）卡尔曼增益计算：        

<img src="雷达.assets/image-20251023175604327.png" alt="image-20251023175604327" style="zoom:50%;" />

HHH：把状态（如位置、速度）投影到测量空间（如传感器的输出：只测位置）；

HTH^THT：则是“反向传播”测量误差信息到状态空间。

HPk−HT->状态预测误差映射到测量空间后的协方差

**线性卡尔曼总结**：传感器有自身的噪声和坏掉的概率等产生的误差，除了测量值产生的过程误差，同时计算出的预测值也有可能因为显示因素产生误差，在引入噪声符合高斯分布时，可以通过协方差计算得出Pk作为预测误差，R作为传感器本身的噪声误差，由厂家给出，但因为实际中有光线角度等因素，所以还需要调整；测量误差同理，将控制量乘矩阵变化为状态空间，同时因为预测误差在状态空间不一定能用的上，所以用H的变换矩阵将测量空间的误差转换为状态空间内的，再乘HT将状态空间内计算的误差进行修正还原到T时刻的ui测量空间，完整公式是预测值加上增益量乘上测量值减去预测值，增益量可视作预测量的置信度，增益量的计算是状态误差在总误差中的占比

**调参**：

| 理论                                  | 实践                                   |
| ------------------------------------- | -------------------------------------- |
| 厂家可以提供 R 的初始估计（噪声方差） | 实际使用中必须根据实测和运行效果调整 R |

这里适用的情况就是机器人运动轨迹为线性，但在实际中并不常见

#### 实现：

https://zhuanlan.zhihu.com/p/712472138



#### 扩展卡尔曼

https://blog.csdn.net/qq_41204464/article/details/123874233

非常好文章：https://blog.csdn.net/O_MMMM_O/article/details/106078679

非常非常好文章：https://blog.csdn.net/AdamShan/article/details/78265754

泰勒公式是一个用函数在某点的信息描述其附近取值的公式,在**均值处进行一阶泰勒展开**,从而逼近平滑函数

<img src="https://i-blog.csdnimg.cn/blog_migrate/a8ef29cba24440cf4b3ae0be078a1d61.png" alt="img" style="zoom: 67%;" />

其中<img src="https://i-blog.csdnimg.cn/blog_migrate/d0f2a0e575e42cce9eb038434e277187.png" alt="img" style="zoom:50%;" />是展开后得到的一阶偏导项        

**雅可比行列式**：**多变量函数的一阶偏导构成的矩阵**，用于描述输入变化对输出的线性影响，比如坐标变换中，从直角坐标 (*x*,*y*)变换到极坐标 (*r*,*θ*)时，面积微元要从 *d**x**d**y*变为 *r**d**r**d**θ*，即将矩形微元扭曲为弧长乘半径

<img src="雷达.assets/image-20251025143658608.png" alt="image-20251025143658608" style="zoom:50%;" />                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         

计算过程就是，控制输入对于运动模型的每一行求偏导，然后写成一行，依次写三行，就能将非线性系统化成线性，得出雅克比矩阵

<img src="https://i-blog.csdnimg.cn/blog_migrate/0f2db6b62d23dd7d99a6978cc6734edc.png" alt="这里写图片描述" style="zoom:50%;" />

## 定位算法

- **重定位：**https://vincentqin.tech/posts/slam-common-issues-relocalisation/

  ![重定位](https://vincentqin.tech/posts/slam-common-issues-relocalisation/relocalisation.svg)

### AMCL自适应蒙特卡洛定位

https://zhuanlan.zhihu.com/p/126199044

https://blog.csdn.net/p942005405/article/details/108569970

**AMCL是[Adaptive Monte Carlo Localization](https://zhida.zhihu.com/search?content_id=116174826&content_type=Article&match_order=1&q=Adaptive+Monte+Carlo+Localization&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NTU0MzY2OTAsInEiOiJBZGFwdGl2ZSBNb250ZSBDYXJsbyBMb2NhbGl6YXRpb24iLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoxMTYxNzQ4MjYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.48x82L3VaIaiByJWsZJ79UeZn-2f4T0mkOryR8bE_1I&zhida_source=entity)（也即是自适应蒙特卡洛定位）的简称，是基于多种蒙特卡洛融合算法在ROS/ROS2系统中的一种实现**。

蒙特卡洛：是一种思想或方法。举例：一个矩形里面有个不规则形状，怎么计算不规则形状的面积？不好算。但我们可以近似。拿一堆豆子，均匀的撒在矩形上，然后统计不规则形状里的豆子的个数和剩余地方的豆子个数。矩形面积知道的呀，所以就通过估计得到了不规则形状的面积。拿机器人定位来讲，它处在地图中的任何一个位置都有可能，这种情况我们怎么表达一个位置的置信度呢？我们也使用粒子，哪里的粒子多，就代表机器人在哪里的可能性高。

<img src="https://pic4.zhimg.com/v2-bd6a3609c2c30b3a20c7922f20a5e22d_r.jpg" alt="img" style="zoom:50%;" />

### LIO定位算法

LIO的核心在于将激光雷达点云数据与IMU的加速度和角速度数据进行融合。其主要步骤包括：

**点云处理**：对激光雷达采集的点云数据进行去噪、滤波和特征提取。**IMU数据预处理**：校正IMU的偏置误差、随机噪声等，确保数据的准确性。**点云配准**：通过ICP（迭代最近点）算法对点云进行配准，估计相邻帧之间的运动。**状态估计**：利用卡尔曼滤波器融合IMU和点云配准结果，更新系统的位姿和状态。

LIO的优势在于其对环境光照和纹理的依赖较低，适用于复杂环境（如夜间或浓雾中）的高精度导航。

https://zhuanlan.zhihu.com/p/541228082

## 点云配准算法

https://zhuanlan.zhihu.com/p/690180250

- **刚性转化：**只允许物体整体平移 + 旋转，形状保持不变。
- **非刚性转化：**允许物体发生形变（拉伸、变形、弯曲、扭曲）以达到更精确的对齐。

### PCL技术

**点云库**是一个站在前人点云研究基础之上建立的跨平台开源的C++库，它实现了和点云数据相关的通用性算法和高效的数据结构，例如：**点云的获取、滤波、分割、配准、检索、特征提取、识别、追踪、曲面重建以及可视化等**。它支持不同的操作系统，例如：Linux、MacOS、Windows、Android以及部分实时的嵌入式操作系统。如果说OpenCV是2D信息获取和处理的代表物，那么PCL就是3D信息提取和处理的代表物。依赖的第三方库有Boost、Eigen、FLANN、VTK、CUDA、OpenNI、Qhull、OpenMP。它利用CUDA、OpenMP等高性能计算技术，通过并行化提高程序的实时性；利用**FLANN**（Fast Library for Approximate Nearest Neighbor）构建K近邻搜索操作的架构，其速度也是非常快的。而且在PCL中，**均是用Boost库的共享指针**来传递数据，因此避免多次赋值系统中已存在的数据的需要。

- **FLANN** 是一个用于在高维空间中进行**快速最近邻搜索**的库。本质上是给定一个点，在庞大的点集合中快速找出离它最近的点。

  当使用特征（如FPFH、SHOT）进行物体识别或粗配准时，需要比较两个点特征之间的相似度。这些特征本身就是高维向量。FLANN 可以快速在高维特征空间中找到最匹配的特征点。

https://blog.csdn.net/CFH1021/article/details/121249157

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/22ce98494254642699c8e953de15b8fa.png#pic_center)

### ICP算法

全名为**迭代最近点算法**，**是一种用于对齐两组点云数据的算法，它的目标是找到两个点云之间的** **最佳刚体变换（旋转 + 平移）**即优化位姿，使得这两组点云尽可能重合（对齐）。对齐方式有关键点检测等多种算法

| 作用                         | 说明                                                         | 应用场景                                                     |
| :--------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **点云对齐（配准）**         | 将不同时间、不同视角下的点云数据（比如激光雷达扫描）对齐到同一个坐标系下 | 激光 SLAM、三维重建、多帧点云拼接                            |
| **位姿估计（相对运动估计）** | 通过匹配当前点云和参考点云（如上一帧、地图），计算出机器人或传感器的 **相对位移和旋转（即位姿变化）** | 机器人定位、里程计替代、SLAM 中的 scan matching              |
| **辅助 SLAM 定位**           | 在没有 GPS、没有可靠里程计时，ICP 可以通过激光雷达扫描匹配来估计机器人当前的位置 | Laser-based SLAM（比如 Gmapping、Cartographer、slam_toolbox） |
| **闭环检测后的精确修正**     | 当 SLAM 系统发现可能回到了之前访问过的位置（闭环），用 ICP 对齐来精确计算位姿差异，从而修正累积误差 | SLAM 后端优化                                                |

-  **ICP 技术的作用是通过匹配点云数据，计算出机器人短时间内的精准位姿变换（比如移动了多少），主要用于局部定位与扫描匹配；而图优化技术的作用是把多个位姿和它们之间的关系构建成图，并通过全局优化方法消除累计误差，得到更准确、一致的轨迹和地图，主要用于 SLAM 的后端优化。**

https://zhuanlan.zhihu.com/p/104735380

求解最优变换的步骤：（刚性转换）

1. 计算源点云和目标点云质心；
2. 将源点云和目标点云进行转换到质心坐标系；
3. 计算矩阵 H（形式类似“协方差矩阵”）；
4. 对 H 求 SVD 分解，根据公式求得 R∗ ；
5. 根据公式计算 t∗ 。

### NDT算法

NDT算法又称正态分布变换算法。一种用于**点云配准（Point Cloud Registration）** 的技术，它的核心思想是将点云数据划分为一个个网格（Voxel），并在每个网格内用一个概率分布（通常是多元高斯分布）来描述该区域内点云的形状，然后通过优化方法找到使两片点云最佳对齐的位姿（即旋转和平移）。简言之，就是把空间中的点云进行整理，整理成具有分布特性的块，然后根据这些块来对不同帧的点云做匹配，求解这些分布块的位姿变换。

- **源点云（Source）**：通常为当前时刻的激光雷达扫描数据
- **目标点云（Target）**：通常为上一帧点云、局部地图或者全局地图

https://zhuanlan.zhihu.com/p/483900570

### 其他算法

small_gicp 是一个**高性能、并行化、几乎零依赖的点云配准（ICP/GICP/VGICP）C++ 库**，用于快速完成 LiDAR 点云间的精确位姿计算。

- 原理：

https://blog.csdn.net/shoufei403/article/details/102972842

- 实现：

https://github.com/ShiJindong/ICP-Methods-Comparision

## 决策树

<img src="雷达.assets/image-20250814182039829.png" alt="image-20250814182039829" style="zoom: 33%;" />

<img src="雷达.assets/image-20250814194158011.png" alt="image-20250814194158011" style="zoom:33%;" />

概述：https://zhuanlan.zhihu.com/p/133838427

原理讲解：https://blog.csdn.net/GreenYang5277/article/details/104500739

具体实现不太了解

## 位姿优化技术

### logistic回归

**Logistic 回归是一种用于解决二分类问题的统计学习方法。它通过一个 sigmoid 函数，将线性回归的输出映射到 [0, 1] 区间，从而表示某个样本属于某一类别的概率。**即在导航中遇到岔路时计算代价

<img src="雷达.assets/image-20250814194700437.png" alt="image-20250814194700437" style="zoom:33%;" />

- **为什么slam不能实时显示地图？**

  SLAM Toolbox 默认依赖 **base_link 相对于 odom 的运动** 来决定每帧 scan 的 pose。如果 odom 不动，SLAM Toolbox 会认为机器人没有移动：

  - scan 匹配不到新的位姿

  - 地图停留在第一帧

    **解决方法：**如果有 IMU 或 VIO/光学跟踪设备：

    - 每帧用实际位姿更新 x_, y_, th_
    - 发布 odom → base_link TF

  <img src="https://i-blog.csdnimg.cn/blog_migrate/3d54ed5dad0ac3117d6547d5a29beb6e.png" alt="在这里插入图片描述" style="zoom: 50%;" />

### 图优化技术

相关文章(https://zhuanlan.zhihu.com/p/362600811)这个包含点对点的位姿矫正

​				(https://blog.csdn.net/datase/article/details/78473804)更易懂and第二篇难

**图优化（Graph Optimization）** 是一种 **把机器人的位姿和约束建模成图结构，并通过数学优化方法（如非线性最小二乘）来估计最优位姿和地图** 的技术。

<img src="https://pic2.zhimg.com/v2-24d9d58d788e8ecd3cf022b879d0d153_1440w.jpg" alt="img" style="zoom:50%;" />

| 作用                     | 说明                                                         | 应用场景                                |
| :----------------------- | :----------------------------------------------------------- | :-------------------------------------- |
| **全局位姿优化**         | 把机器人在不同时间点的位姿作为图的节点，把位姿之间的关系（比如运动约束、匹配约束）作为边，通过优化整张图，得到全局一致的、误差最小的位姿估计 | SLAM 后端优化、消除累计误差             |
| **消除累计误差（漂移）** | 单次定位（比如里程计、ICP）可能有小误差，随着时间推移会累积。图优化通过全局调整，减少这些误差，让轨迹更准 | 长时间运行的机器人导航与建图            |
| **闭环检测后的优化**     | 当检测到“我回到了之前的位置”，图优化可以基于这个信息，重新调整之前所有位姿，修正地图和轨迹 | SLAM 中的闭环校正                       |
| **融合多传感器约束**     | 可以把来自 ICP、里程计、IMU、视觉等不同传感器的约束都加入图中，统一优化 | 多传感器融合 SLAM（比如 LIO-SAM、VINS） |

### 建图技术

https://blog.csdn.net/qq_40695642/article/details/128472360

| 技术                              | 类型            | 传感器          | 是否需要里程计 | 主要特点                  | 适用场景             |
| :-------------------------------- | :-------------- | :-------------- | :------------- | :------------------------ | :------------------- |
| **gmapping**                      | 2D 激光 SLAM    | 2D 激光雷达     | 需要           | 经典粒子滤波，ROS常用     | 室内轮式机器人       |
| **hector_slam**                   | 2D/3D 激光 SLAM | 激光雷达        | 不需要         | 扫描匹配，不依赖里程计    | 无人机、无里程计平台 |
| **karto**                         | 激光 SLAM       | 激光雷达        | 通常需要       | 图优化，精度高            | 大范围建图           |
| **cartographer**                  | 2D/3D 激光 SLAM | 激光雷达        | 可无，推荐有   | Google出品，图优化强      | 大场景、高鲁棒性     |
| **rgbdslam**                      | RGB-D SLAM      | RGB-D相机       | 不需要         | 稠密3D建图，早期流行      | 小范围稠密地图       |
| **ORB-SLAM2 / 3**                 | 视觉SLAM        | 单目/双目/RGB-D | 不需要         | 特征点法，精度高，支持IMU | 视觉导航、AR/VR      |
| **loam / LeGO-LOAM**              | 3D激光SLAM      | 3D激光雷达      | 不需要         | 实时性强，特征匹配        | 无人车、无人机       |
| **三维重建（如Meshroom/COLMAP）** | SfM / MVS       | RGB图像         | 不需要         | 从照片中重建稠密3D模型    | 文物、建筑、VR内容   |

### cartographer算法

**子图构建（Submap）**：将扫描数据分批次保存到子图中，每个子图只覆盖环境的一部分。

**回环检测（Loop Closure）**：检测机器人回到之前走过的地方，修正累积误差。

**位姿图优化（Pose Graph Optimization）**：用图优化方法（比如 Ceres Solver）全局调整机器人的轨迹和地图，减少漂移。

- **用码盘（编码器）数据作为里程计（odometry）**
  1. **读取编码器数据**（脉冲数）
  2. **计算位移与速度**（根据轮半径、编码器分辨率、轮距）
  3. **发布里程计消息**（ROS 里就是 `/odom` 话题）

​		我感觉就是以码盘代替仿真中的差速控制器

![img](https://i-blog.csdnimg.cn/blog_migrate/b48e5cfc5f7cafe2d1e5aa6fe1f65750.png)

#### 拓扑

**用来描述环境中**各位置之间连接关系的抽象图结构

- 表示关键节点（关键帧、房间、路口等）
- 表示节点之间的可通行关系（边）
- 提供高层导航（从 A 到 B 要经过哪些节点）

| 类型                                 | 描述                         | 适用场景                  | 优点                       | 缺点                 |
| ------------------------------------ | ---------------------------- | ------------------------- | -------------------------- | -------------------- |
| **邻接表（Adjacency List）**         | 每个节点记录其相邻节点列表   | 稀疏图、SLAM拓扑地图      | 占用内存低，查找相邻节点快 | 不直观，不适合稠密图 |
| **邻接矩阵（Adjacency Matrix）**     | 用矩阵存储任意两节点是否连接 | 稠密图、小图              | 结构简单、直观             | 空间复杂度高 O(n²)   |
| **边列表（Edge List）**              | 单独列出所有边               | 图优化、Graph SLAM        | 边信息统一易管理           | 找相邻节点时不方便   |
| **带权图（Weighted Graph）**         | 边带有权重，如距离、误差     | Pose Graph SLAM、路径规划 | 能表示约束强弱或代价       | 计算更复杂           |
| **有向图（Directed Graph）**         | 边有方向，如 A→B             | 单行道、流程图            | 能表示方向性约束           | 结构更复杂           |
| **无向图（Undirected Graph）**       | 边无方向                     | 常见路径连通、GMapping    | 表示简单，适用广           | 无法表示单向约束     |
| **拓扑语义图（Semantic Topology）**  | 节点带语义（房间、走廊）     | 语义导航                  | 高层规划能力强             | 构建成本高           |
| **层级拓扑图（Hierarchical Graph）** | 分层，如房间–走廊–建筑层级   | 大场景导航                | 可扩展性强                 | 建设复杂             |

## 地图类型

- **什么是占用栅格地图？**

  | 特点                 | 说明                                                         |
  | :------------------- | :----------------------------------------------------------- |
  | **2D 表示**          | 通常用于 **二维平面环境**（如室内、仓库、走廊等）            |
  | **概率性表示**       | 每个格子是“被占用”的概率，不是绝对的“有/无”                  |
  | **适合导航**         | 路径规划算法（如 A*、Dijkstra、DWA 等）通常直接使用这种地图  |
  | **由 SLAM 算法生成** | 很多 2D 激光 SLAM 算法（如 gmapping、cartographer）都会输出占用栅格地图 |
  | **可存储 / 可视化**  | 可保存为图片、PGM/PBM 文件，或 ROS 地图服务器中供导航使用    |

| 地图类型             | 表示方式          | 维度    | 是否栅格化   | 是否依赖传感器   | 典型应用              |
| :------------------- | :---------------- | :------ | :----------- | :--------------- | :-------------------- |
| **1. 占用栅格地图**  | 概率栅格（0~100） | 2D      | 是           | 激光雷达         | 室内导航、ROS建图     |
| **2. 八叉树地图**    | 3D 栅格（概率）   | 3D      | 是（树结构） | 激光/RGB-D       | 无人机避障、3D导航    |
| **3. TSDF/体素地图** | 带符号距离场      | 3D      | 是（体素）   | RGB-D / 深度相机 | 表面重建、三维建模    |
| **4. 稠密点云地图**  | 3D点集合（x,y,z） | 3D      | 否           | 激光雷达 / RGB-D | 高保真建图、重建      |
| **5. 稀疏点云地图**  | 特征点/关键点     | 3D      | 否           | LiDAR / 视觉     | 定位、LiDAR SLAM      |
| **6. 特征点地图**    | 特征描述子 + 位置 | 2D / 3D | 否           | 相机             | 视觉SLAM、轻量定位    |
| **7. 拓扑地图**      | 节点 + 连接关系   | 2D / 3D | 否           | 抽象             | 高层导航、任务规划    |
| **8. 语义栅格地图**  | 栅格 + 语义标签   | 2D / 3D | 是           | 相机/激光+识别   | 智能导航、服务机器人  |
| **9. 代价地图**      | 栅格 + 代价值     | 2D      | 是           | 通常基于占用栅格 | 导航与路径规划（ROS） |

### 鲁棒性

**鲁棒性（Robustness）** 是指一个系统、算法、模型或组件在面对**不确定性、错误输入、异常情况、外部干扰或部分失效**时，依然能够**保持稳定运行并维持预期性能**的能力

### 代价计算

将普通坐标转换为栅格坐标后计算概率

**栅格坐标（Grid Coordinates / Cell Coordinates / Map Index）**

- **含义**：是 **二维数组索引**，表示某个点在 **代价地图（Costmap）或地图图像中对应的栅格（Cell）的位置**。
- **坐标系**：以 **栅格（格子）为单位**，每个格子代表地图上的一个固定大小的区域（比如 0.05m × 0.05m）。
- **表示方式**：通常是 `(mx, my)`，两个整数，分别表示 **行（y方向）和列（x方向）的索引号**。

## 路径规划算法

导航我理解的目前原理就是规划器代价计算概率同时遍历所有点，就所有点生成一个最短路径，然后再把不用的点和未来点扔掉返回最近的需要点，通过pose发布给控制器，确认接收后在返回变化定下一个点(动作服务器)，然后控制器计算到目标点的步进和朝向逻辑最后返回线速度和角速度分装的twist消息，再有控制器内置的自动转换为话题的工具，发布为/cmd。主机可以通过订阅接收，或者直接定义全局变量取上坐标通过串口返回，所以可以自由选择要给主机传入的东西，而且他们通过.yaml参数启动，而不是launch的显式启动

https://zhuanlan.zhihu.com/p/364421182

### 可视图算法(Visibility Graph)

一种经典的路径规划方法。其原理是：将路径的起点、终点以及所有障碍物的各个顶点作为图中的**节点**。如果两个节点之间可以直接“看见”对方（即连接它们的线段不穿过任何障碍物），则在这两个节点之间画一条边。

https://blog.csdn.net/FRIGIDWINTER/article/details/146117544

### 图搜索算法

https://zhuanlan.zhihu.com/p/346666812

![img](https://pic1.zhimg.com/v2-fe94f576e43e54b5d11d51e1ceb376fe_1440w.jpg)

<img src="雷达.assets/image-20251127120039360.png" alt="image-20251127120039360" style="zoom: 50%;" />

## 点云聚类

- open3D:

  https://blog.csdn.net/SimSolve/article/details/155536498

（不同算法：https://zhuanlan.zhihu.com/p/497930938

基本概念：https://blog.csdn.net/Yong_Qi2015/article/details/114909485）

基于划分的聚类算法，针对散落的点集，选定聚类个数然后随机初始化中心点，通过迭代计算拉近类内点的距离，增大类间点的距离。

基于网格的聚类是将数据空间划分成栅格形式，将数据映射到网格中，然后计算每个栅格中点集密度，根据预设的阈值对网格单元的类别作出分类，并与邻近的栅格组形成类。



![img](https://i-blog.csdnimg.cn/blog_migrate/a84917e23e6aa3a07b4fcc2ec9b3bc97.png)

### 3D目标检测

**框架：**

https://zhuanlan.zhihu.com/p/569189196

- **OpenPCDet:**

  OpenPCDet 是目前最主流的 3D 点云检测开源框架之一，专注于 **基于 LiDAR 的 3D 目标检测**。它结构清晰、模块化程度高，提供大量经典模型如 SECOND、PointPillars、PV-RCNN、CenterPoint 等。其最大的优势是：**学术友好、代码易读、复现准确、模型种类全**

  讲解：https://zhuanlan.zhihu.com/p/152120636

  开源：https://github.com/open-mmlab/OpenPCDet

  ![img](https://pic3.zhimg.com/v2-36020bacb0c634a1382f367bff7cd562_1440w.jpg)

- **mmdetection3d:**

  MMDetection3D 是 OpenMMLab 系列中的 3D 感知框架，覆盖 **点云、相机、融合** 三种模式的目标检测、语义分割、深度估计等任务。它支持从单目、多目、LiDAR 到融合的统一训练流程，生态丰富、模块高度可复用，并整合了 MMEngine 的训练调度系统。

- **Det3D: **

  Det3D 是一个更轻量的 3D 检测框架，专注于 **自动驾驶场景** 中的 3D 目标检测（尤其是 PointPillars 与 CenterPoint）。整体代码简洁、速度快、部署方便，适合对性能和实时性有需求的工程端使用。不过框架更新较少，生态比不上 MMDetection3D 和 OpenPCDe

- **Paddle3D**:

  Paddle3D 是百度基于 PaddlePaddle 推出的 3D 感知平台，特点是模型部署友好，特别适合在国产硬件（如昆仑芯、昇腾）上优化部署。它支持点云检测、三维重建、表征学习等任务。优势在于 **推理引擎优化好、部署效率高**，但学术生态和模型数量相对较少

- ---------------**数据集**----------------

  | 数据集         | 原生格式                  |
  | -------------- | ------------------------- |
  | KITTI          | `.bin` 点云 + `.txt` 标签 |
  | Waymo          | `.tfrecord`               |
  | NuScenes       | `.pcd`/`.bin` + JSON      |
  | custom dataset | 用户自己决定              |

**算法：**

https://zhuanlan.zhihu.com/p/591349104

<img src="https://pic3.zhimg.com/v2-b5641b02ab9704ae7a83fedc4359c4fa_1440w.jpg" alt="img" style="zoom:200%;" />

最优的融合算法：https://blog.csdn.net/wqwqqwqw1231/article/details/104250027

### 语义分割

详细可以自行了解

**框架：**

https://zhuanlan.zhihu.com/p/701605684

![img](https://picx.zhimg.com/v2-7121bdb4d2b47f748092b2951f2bbff1_1440w.jpg)

**算法：**

https://zhuanlan.zhihu.com/p/591349481

<img src="https://pic3.zhimg.com/v2-2fc162a22f3909ff555883b002b3e798_1440w.jpg" alt="img" style="zoom:200%;" />

### 数据集对比

**--------------------类别简述--------------------**

**KITTI 数据集**

- **用途**: 自动驾驶场景下的 3D 目标检测
- **传感器**: 激光雷达 + 单目相机
- **目标类别**: Car(汽车)、Pedestrian(行人)、Cyclist(骑车人)
- **特点**: 最经典的自动驾驶数据集之一，提供了 velodyne 点云数据

**Waymo 数据集**

- **用途**: 大规模自动驾驶 3D 目标检测
- **传感器**: 多激光雷达 + 多摄像头
- **目标类别**: Vehicle(车辆)、Pedestrian(行人)、Cyclist(骑车人)、Sign(标志牌)等
- **特点**: 比 KITTI 更大规模，包含更多传感器数据

**nuScenes 数据集**

- **用途**: 多模态自动驾驶感知任务
- **传感器**: 360度激光雷达 + 6个摄像头 + 其他传感器
- **目标类别**: 10种类别包括 car、truck、bus、pedestrian 等
- **特点**: 覆盖更多目标类别，时间序列标注

**Lyft 数据集**

- **用途**: 自动驾驶 3D 目标检测挑战
- **传感器**: 激光雷达为主
- **目标类别**: 类似于 Waymo 和 KITTI
- **特点**: 提供丰富的 3D 边界框标注



**--------------不同数据集对比---------------**

| 特性           | KITTI             | Waymo               | nuScenes         | Lyft       |
| :------------- | :---------------- | :------------------ | :--------------- | :--------- |
| **数据量**     | 中等(~7000帧训练) | 大量(>100000帧训练) | 大量(>34000场景) | 大量       |
| **传感器数量** | 单激光雷达+单相机 | 多激光雷达+多相机   | 多传感器融合     | 多激光雷达 |
| **目标类别数** | 3类               | 5类                 | 10类             | 4类        |
| **主要应用**   | 基础研究          | 大规模训练          | 多任务评估       | 挑战赛     |

### 模型对比

**PointPillar模型特点**

架构特征：

- **基于点云的柱状表示**：将点云转换为伪图像表示(Pseudo-LiDAR representation)
- **轻量级网络**：计算效率高，适合实时应用
- **端到端训练**：从点云到3D框预测一体化处理

技术优势：

- 将3D点云划分为垂直柱状结构(vertical pillars)
- 使用MLP编码每个柱状内的点特征
- 通过2D卷积网络处理生成的柱状特征图
- 速度快，适合部署在资源受限环境

**PV-RCNN模型特点**

架构特征：

- **Point-Voxel融合**：结合点级和体素级特征处理
- **两阶段检测器**：类似Faster R-CNN的架构设计
- **复杂的特征提取**：包含多个处理阶段

技术优势：

- 使用VoxelNet进行初步特征提取
- 引入RoI-grid池化机制提取精确的局部特征
- 采用两阶段proposal refinement提高精度
- 检测精度更高，但计算复杂度也更大

**主要区别对比**

| 特征           | PointPillar        | PV-RCNN        |
| :------------- | :----------------- | :------------- |
| **模型复杂度** | 简单，轻量级       | 复杂，多阶段   |
| **推理速度**   | 快                 | 较慢           |
| **检测精度**   | 中等               | 高             |
| **内存占用**   | 低                 | 高             |
| **适用场景**   | 实时应用、边缘设备 | 精度优先的应用 |
| **参数量**     | 较少               | 较多           |

### 训练模式对比

**1. 分布式训练 (Distributed Training)**

对应脚本: [dist_train.sh](javascript:void(0))

```
bash

python -m torch.distributed.launch --nproc_per_node=${NGPUS} --rdzv_endpoint=localhost:${PORT} train.py --launcher pytorch ${PY_ARGS}
```

特点:

- 使用 `torch.distributed.launch` 模块启动
- 在单机多GPU环境下运行
- 需要手动指定可用端口
- 直接调用 [train.py](javascript:void(0)) 脚本

**2. SLURM 集群训练 (SLURM Cluster Training)**

对应脚本: [slurm_train.sh](javascript:void(0))

```
bashsrun -p ${PARTITION} \
    --job-name=${JOB_NAME} \
    --gres=gpu:${GPUS_PER_NODE} \
    --ntasks=${GPUS} \
    --ntasks-per-node=${GPUS_PER_NODE} \
    --cpus-per-task=${CPUS_PER_TASK} \
    --kill-on-bad-exit=1 \
    ${SRUN_ARGS} \
    python -u train.py --launcher slurm --tcp_port $PORT ${PY_ARGS}
```

特点:

- 专为 SLURM 集群环境设计
- 使用 `srun` 命令提交作业
- 可以指定分区(partition)、CPU数量等资源参数
- 支持更细粒度的资源配置控制
- 通过 `--launcher slurm` 参数告知程序运行环境

**3. TorchRun 训练 (TorchRun Training)**

对应脚本: [torch_train.sh](javascript:void(0))

```
bash

torchrun --nproc_per_node=${NGPUS} --rdzv_endpoint=localhost:${PORT} train.py --launcher pytorch ${PY_ARGS}
```

特点:

- 使用新版 `torchrun` 命令(替代 `torch.distributed.launch`)
- 更简洁的命令语法
- 自动处理许多分布式训练的细节
- 是 PyTorch 推荐的分布式训练启动方式

**对比总结**

| 特性       | Distributed ([dist_train.sh](javascript:void(0))) | SLURM ([slurm_train.sh](javascript:void(0))) | TorchRun ([torch_train.sh](javascript:void(0))) |
| :--------- | :------------------------------------------------ | :------------------------------------------- | :---------------------------------------------- |
| 运行环境   | 单机多GPU                                         | 集群环境(SLURM调度器)                        | 单机多GPU                                       |
| 启动命令   | `python -m torch.distributed.launch`              | `srun`                                       | `torchrun`                                      |
| 资源管理   | 基础GPU分配                                       | 细粒度资源控制(CPU/GPU/分区)                 | 基础GPU分配                                     |
| 现代化程度 | 较老的方式                                        | 集群专用方案                                 | 最新推荐方式                                    |
| 配置复杂度 | 简单                                              | 复杂                                         | 简单                                            |

每种方式适用于不同的场景：本地开发调试适合使用 `torchrun` 或 `distributed` 方式，而大规模训练任务则更适合在 SLURM 集群上运行。

### BEVFusion 

**BEVFusion 是一个多模态融合方法**
 将 **摄像头（Camera）** + **激光雷达（LiDAR）** 的信息统一转成 **鸟瞰图 BEV（Bird-Eye-View）** 表示，再进行融合和检测。

1. **不同模态 → 转换到同一 BEV 空间**
   - 摄像头冻结 → 先把多视角图像用 Transformer / CNN 提取特征
   - 使用深度估计 + 几何映射把图像特征投射到 BEV
   - 激光雷达体素化 → 直接生成 LiDAR BEV 特征
2. **在 BEV 上进行融合（BEV-level Fusion）**
   - 将 LiDAR BEV + Camera BEV 做融合（加法、拼接、注意力等）
   - 因此不用逐点融合，计算量更小，空间一致性更强
3. **再在 BEV 上做 3D 目标检测**
   - BEV 是 2D 的，所以效率更高
   - 检测车、人、障碍物、tracking、地图等任务效果更好

BEVFusion = 将图像特征映射到 BEV + 将 LiDAR 特征转成 BEV + 在 BEV 上融合检测。

### MPPNet

**MPPNet** 全称 **Multi-frame Progressive Pose Network**，是 **Waymo Open Dataset 3D 检测任务的冠军方法之一（2022）**。

它是一种 **多帧点云融合 + 逐步增强检测精度** 的 3D 目标检测模型，主要用于自动驾驶场景。

**1. 远距离目标太稀疏 → 难检测**

单帧点云距离大时变得稀疏，车、人等目标点数极少。

➡ MPPNet 使用 **多帧融合**（Multi-frame）将前几帧点云融合到当前时刻，让远处物体的点变得更密。

**2. 多帧融合会导致物体位置变化（运动模糊）**

目标在移动，多帧直接叠加会产生重影。

➡ MPPNet 使用 **progressive pose warping（逐层位姿扭正）**
 把历史帧点云根据车辆自车位姿 + 预测目标位姿 **动态对齐**，让物体在新时刻的位置更加准确。

**3. Temporal Fusion Head（时序融合检测头）**

融合后的特征送入 BEV 的检测头，预测：

- bounding box
- 速度 velocity
- 朝向 orientation
- 类别 cls

MPPNet 的检测头轻量、简洁、效率高。

### 模型yaml参数



# ==强化学习==

https://blog.csdn.net/lovely_yoshino/article/details/154728559

# ==雷达分类==

## 2D雷达

### 轮趣科技雷达

### ——镭神M10P

windows上位机看点云图

**串口通信**

1.  轮趣的镭神雷达里面有一个串口转接模块，就是在雷达盒子里的那个比较小的那个转接模块

2.  把雷达的线插进去

    像这样：

3.  随便找一个比较粗的usb->type-c线（比如手机充电线）一边连电脑自带的usb（必须是自带的，别连在拓展坞上，主要是供电不足），一边连串口转接模块

**网口通信（以太网通信）**

步骤 1：进入你的 ROS 2 工作空间

```
cd ~/ros2_ws/src
```

步骤 2：克隆 leishen_driver 仓库

**GitHub 镜像地址**（常用，适配 ROS 2）：

```
git clone https://github.com/LeishenLidar/Leishen_ROS2_Driver.git
```

或国内镜像（如访问较慢）：

```
git clone https://gitee.com/leishen_lidar/Leishen_ROS2_Driver.git
```

步骤 3：编译驱动包

返回工作空间根目录并编译：

```
cd ~/ros2_ws
colcon build --packages-select leishen_lidar_sdk
```

> 注意：有时驱动包名叫 `leishen_lidar_sdk`，也有叫 `lslidar_driver` 的旧版本，取决于你克隆的仓库。

### 锐驰智光雷达

### —Lakibeam1

-   官网先奉上：https://www.richbeam.com/download

windows上位机看点云图：

-   官网先奉上：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2025-06-05/cck4ydmasmbtvfe0q7wpiaw60w93hcqn_.pdf
-   B站视频奉上：【锐驰智光LakiBeam单线系列激光雷达操作教程（Windows版）】https://www.bilibili.com/video/BV1LN4y1w7WV?vd_source=a149de35c7ad820661d6f346b01b0eb1
-   自带rg45网口和dc电源接口

**网口通信**（以太网通信）

1.  网口通信也是一样的供电支持9到36V，还是和镭神一样。直接用队里的充电宝或者12V的电池供电
2.  雷达的接口是DC55-21供电接口和一个RJ45以太网口，使用雷达的时候一个供电，一个通过网线连到主机或电脑上就行

----------------------------------------------------------------------------------------------------------------------------

1.  雷达的串口仅支持5V供电，轮趣的镭神雷达也是5V供电
2.  然后看type-c接口的位置：把两个梅花六角螺丝拧下来里面就是type-c接口

**linux上用ROS2跑雷达**

-   官网先奉上：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2024-05-23/q26paa5fm40da9cbjp2roz4ihgs2vyf6_.pdf

1. ###### 网络IP设置

   -   查找网络接口`ip a`，查找雷达虚拟网卡，如`enxaaa8d18544d7`

   -   启动虚拟网卡`sudo ip link set enxaaa8d18544d7 up`

   - 进入http://192.168.8.2/config.html，设置dhcp动态分配地址`192.168.8.1`

     - 这里如果是usb-typec就这么设置,pcl以太网的话要设为`192.168.198.1`

   - 运行`ping 192.168.8.2`，显示ping通即可

     - 同理，rg45这里ping`192.168.198.2`

   -   但是这样做会有一个缺点，就是每次重启都得设置一次，有点麻烦，可以这样：在终端里运行

       ```python
       sudo nano /etc/netplan/01-network-manager-all.yaml
       ```

       然后Ctrl+o进入写入模式，修改为以下内容：

       ```python
       network:
         version: 2
         renderer: NetworkManager
         ethernets:
           enp57s0:
             addresses: [192.168.198.1/24]
             dhcp4: no
             optional: true
       ```

       运行`sudo network apply`应用

4. ###### 跑代码

   -   点进去，下载驱动（也可以说是代码）：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2025-04-14/dkup0izne6ocyz49x2bmvv61ehpobgdq_.zip

   -   创建一个文件夹（用来放驱动），在文件夹里面创建src文件夹，把下载好的驱动提取到src文件夹里，在src的上级目录下打开终端，运行`colcon build`构建驱动。应该会构建8到30秒

   -   然后就可以运行驱动了：

       更新路径：

       ```python
       source ./install/setup.bash
       ```

       运行驱动：

       ```python
       ros2 launch lakibeam1 lakibeam1_scan.launch.py
       ```

       **然后这里注意了：**运行驱动后，终端里面会不断地显示数据，这才是正常现象，如果卡在一个地方，说明出问题了

       然后可能出的问题有两个：

       -   防火墙
       
       -   以太网口有多个IP（导致驱动检测到的数据不全是雷达的，然后卡在一个地方一直卡着）

           解决方法：只抓取和雷达相关的信息：

           ```python
           sudo tcpdump -i eth0 -n host 192.168.198.2
           ```
       
           没安装先安装：
       
           ```python
           sudo apt update
           sudo apt install tcpdump
           ```
       
       
       - 驱动中的remote.cpp作用：
       
         通过 HTTP PUT 请求，向传感器设备（如 LiDAR）发送配置参数，比如设置扫描频率、滤波参数等，通过多个 HTTP GET 请求，获取传感器设备的各种状态信息，比如型号、版本、运行负载、电机转速、扫描范围等，并通过 ROS2 打印日志，通信方式是使用 **libcurl** 发送 HTTP 请求，与传感器设备进行 **RESTful API 通信**
       
         ###### 什么是libcurl,cURL,curl?
       
         <img src="https://i-blog.csdnimg.cn/blog_migrate/dfeb4a7f8724702ae3630d45d38ea60e.png" alt="在这里插入图片描述" style="zoom:50%;" />
       
       新建终端，然后更新路径：
       
       ```python
       source ./install/setup.bash
       ```
       
       查看点云图（在Rviz里面）
       
       ```python
       ros2 launch lakibeam1 lakibeam1_scan_view.launch.py
       ```
   
   **总结一下：**设置ip这个操作主要是为了设备上收集到数据后可以通过主机的scan话题发布，但通过不同线缆连接后以太网协议是不一样的，网卡方面雷达自带的虚拟网卡就很方便不用安插更多的东西

## 3D雷达

### 觅道雷达

### ——MID-360

- 官网地址：https://terra-1-g.djicdn.com/65c028cd298f4669a7f0e40e50ba1131/Mid360/Livox_Viewer_2_User_Manual_chs_v1.2.pdf
- 工作电压：12v
- 连接上有线网之后，分配地址192.168.1.50掩码255.255.255.0

https://www.bilibili.com/opus/986664810984767490按这个来就行，注意扫一下雷达二维码查序列号，得出它的自身ip地址

# ==建图方法==

## **gmapping**



<img src="https://pica.zhimg.com/v2-74708d344109219df04fd421ec0f3310_1440w.jpg" alt="img" style="zoom:50%;" />

## **cartigrapher**

从原理中我们得知，它比起其他算法带有回环检测功能，并且不一定需要imu即odem反馈数据，很适合测试雷达时手持建图，解决了没有里程计卡在第一帧的问题

- 在官驱config下创建.iua文件

  ```
  include "map_builder.lua"
  include "trajectory_builder.lua"
  options = {
    map_builder = MAP_BUILDER,
    trajectory_builder = TRAJECTORY_BUILDER,
    map_frame = "map",
    --  跟踪和发布的frame都改成雷达的frameID
    tracking_frame = "laser",
    published_frame = "laser",
    odom_frame = "odom",
    -- true改为false，不用提供里程计数据
    provide_odom_frame = false,
    -- false改为true，仅发布2D位资
    publish_frame_projected_to_2d = false,
    use_pose_extrapolator = true,
    -- true改为false，不使用里程计数据
    use_odometry = false,
    use_nav_sat = false,
    use_landmarks = false,
    -- 0改为1,使用一个雷达
    num_laser_scans = 1,
    -- 1改为0，不使用多波雷达
    num_multi_echo_laser_scans = 0,
    -- 10改为1，1/1=1等于不分割
    num_subdivisions_per_laser_scan = 1,
    num_point_clouds = 0,
    lookup_transform_timeout_sec = 0.2,
    submap_publish_period_sec = 0.3,
    pose_publish_period_sec = 5e-3,
    trajectory_publish_period_sec = 30e-3,
    rangefinder_sampling_ratio = 1.,
    odometry_sampling_ratio = 1.,
    fixed_frame_pose_sampling_ratio = 1.,
    imu_sampling_ratio = 1.,
    landmarks_sampling_ratio = 1.,
  }
  -- false改为true，启动2D SLAM
  MAP_BUILDER.use_trajectory_builder_2d = true
  -- 0改成0.10,比机器人半径小的都忽略
  TRAJECTORY_BUILDER_2D.min_range = 0.10
  -- 30改成3.5,限制在雷达最大扫描范围内，越小一般越精确些
  TRAJECTORY_BUILDER_2D.max_range = 5.5
  -- 5改成3,传感器数据超出有效范围最大值
  TRAJECTORY_BUILDER_2D.missing_data_ray_length = 3.
  -- true改成false,不使用IMU数据，大家可以开启，然后对比下效果
  TRAJECTORY_BUILDER_2D.use_imu_data = false
  -- false改成true,使用实时回环检测来进行前端的扫描匹配
  TRAJECTORY_BUILDER_2D.use_online_correlative_scan_matching = true 
  -- 1.0改成0.1,提高对运动的敏感度
  -- TRAJECTORY_BUILDER_2D.motion_filter.max_angle_radians = math.rad(0.1)
  -- 0.55改成0.65,Fast csm的最低分数，高于此分数才进行优化。
  POSE_GRAPH.constraint_builder.min_score = 0.65
  --0.6改成0.7,全局定位最小分数，低于此分数则认为目前全局定位不准确
  POSE_GRAPH.constraint_builder.global_localization_min_score = 0.7
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.linear_search_window = 0.1
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.translation_delta_cost_weight = 10.
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.rotation_delta_cost_weight = 1e-1
  POSE_GRAPH.optimization_problem.huber_scale = 1e2
  POSE_GRAPH.optimize_every_n_nodes = 35
  -- 设置0可关闭全局SLAM
  -- POSE_GRAPH.optimize_every_n_nodes = 0
  return options
  ```

- 运用官方库编写launch

  ```
  from launch import LaunchDescription
  from launch_ros.actions import Node
  
  def generate_launch_description():
      return LaunchDescription([
          Node(
              package='cartographer_ros',
              executable='cartographer_node',
              name='cartographer_node',
              output='screen',
              parameters=[{
                  'use_sim_time': False
              }],
              arguments=[
                  '-configuration_directory', '/home/wybie/lakibeam1_ENGINE/src/Lakibeam_ROS2_Driver-main/config',
                  '-configuration_basename', 'backpack_2d.lua'
              ],
              remappings=[('/scan', '/scan')]
          ),
          Node(
              package='cartographer_ros',
              executable='cartographer_occupancy_grid_node',
              parameters=[{'resolution': 0.05}],
              remappings=[('/map', '/map')]
          )
      ])
  
  ```

  - `use_sim_time=false`需要实时建图不需要仿真时间时，这个要关掉

以上，相比原理来说实现相对简单得多，lua文件有开源的格式和讲解，launch中也只需要调用算法库文件的节点就能解决

## slam-toolbox

**运行命令**

```
ros2 launch slam_toolbox online_async_launch.py use_sim_time:=True \
  slam_params_file:=/home/wybie/lakibeam1_ENGINE/src/Lakibeam_ROS2_Driver-main/config/slam_config.yaml \
  log_level:=debug
```

**slam_config.yaml**

```
slam_toolbox:
  ros__parameters:
    use_sim_time: false        # 如果不在仿真或 rosbag
    slam_mode: true            # 运行 SLAM
    resolution: 0.05           # 地图分辨率，单位 m
    map_update_interval: 1.0   # 地图更新周期，单位 s
    max_laser_range: 20.0      # 雷达最大检测距离

    # 坐标系
    odom_frame: "odom"          # 没有里程计时，暂时让 odom = map
    map_frame: "map"
    base_frame: "base_link"    # 上面 static TF 发布的 base_link
    scan_topic: "/scan"        # 雷达话题

    # 发布 TF
    publish_tf: true
    map_publish_period: 1.0

    # 其它参数
    minimum_time_interval: 0.5
    transform_publish_period: 0.05
    enable_interactive_mode: false
    use_scan_matching: true     # 没有里程计时必须打开
```

# ==一些知识==

## rosdep

小鱼开发的一个工具，使用前执行以下命令

```
sudo rosdep init --rosdistro humble  # 根据你的ROS2版本修改
rosdep update --include-eol-distros --rosdistro humble
```

## 北极熊

- **导航开源**

  https://bbs.robomaster.com/article/714072?source=1                                                                                                                pid控制器流程：

  1.从 global_plan 中裁剪出 local_costmap 范围内的路径

  2.根据当前速度自适应计算 carrot_pose，即前瞻点

  3.解耦线速度和角速度的计算

  使用平移 PID 和旋转 PID，根据期望值分别输出线速度和角速度，对于 RM 场景可选择仅输出线速度。
  
  ![image-20251126233947827](雷达.assets/image-20251126233947827.png)

- **禁用 `small_gicp`**：`small_gicp`可能是一种“局部点云配准”或“轻量化扫描匹配”的算法（比如用于实时定位但不建图的场景）。建图模式下，需要更完整的地图构建逻辑，因此禁用它。
- •**发送静态 `tf`（`map->odom`）**：`tf`是机器人坐标系之间的变换（`transform`）。`map`是“世界坐标系”（全局地图的坐标系），`odom`是“里程计坐标系”（基于机器人运动的局部坐标系）。发送**静态**的 `map->odom`变换，意味着假设“地图”和“里程计”的相对位姿是固定的（或已校准），这有助于建图时保持地图的全局一致性。

## 双主机ROS

ros2是分布式通信，通过交换机连接到同一个局域网，然后固定id号，避免话题冲突，就可以实现两个主机跑同一个机器人；关于网络布局可以去学一下什么是拓扑

拓扑参考：https://zhuanlan.zhihu.com/p/703792524

## 识别物体

主要通过**点云聚类**将场景分割成独立个体，然后利用**传统机器学习**或（更主流的）**深度学习**方法对这些个体进行分类。作为**可靠的距离和几何信息提供者**

实现参考openpcdet,聚类原理中有详细说明

## 运动畸变

在机器人运动过程中，每个激光点都在**不同的基准位姿**上产生（就是由于机器人自身的运动，在不同的时刻发出激光时，机器人的位置是不同的，这也是运动畸变产生的根源所在）。

激光扫描时伴随着机器人的运动，每个角度的激光数据都**不是瞬时获得**的，当激光雷达扫描的频率比较低的时候，机器人运动带来的激光帧的运动误差是不能被忽略的。

参考：https://zhuanlan.zhihu.com/p/683126574

## 雷达的一点

**slam框架**

https://zhuanlan.zhihu.com/p/441386977

**外参标定**

https://blog.csdn.net/m0_49384824/article/details/142483862

**点云分割**

https://zhuanlan.zhihu.com/p/1899846790242600048

**目标检测小论文**

https://zhuanlan.zhihu.com/p/532803205

**点云格式**

https://blog.51cto.com/u_16186343/7397695

- **sliding window：**
   把最近 k 帧的点云检测结果放在一起看。

  **voting scheme：**
   每帧都对“哪里有车/人”投一票，多个帧的票加起来决定最终检测。

- **3DBBox**

  3D BBox(A three-dimensional bounding box，三维边界框)用于定义目标在3D空间中的位置、大小和方向。3D BBox具有适当的尺寸和方向来与目标紧密绑定。如果目标被部分遮挡或截断，3D BBox调整适当的大小和位置以表示完整目标的大小。

- 3D目标检测器使用都PointNet++作为3DBN来进行特征提取

- PCDet 并不关心：

   你是不是 Velodyne，你是不是 KITTI 官方数据

   它只关心三件事：

   点云张量 + 标注张量 + 坐标约定

# ==话题解析==

## **里程计**

https://blog.csdn.net/m0_73640344/article/details/145838906

有对应的话题与消息类型，可在启动建图的文件中调用，主要目的是做到识别位姿和里程记录，做到导航的实时性

<img src="雷达.assets/image-20251127102359395.png" alt="image-20251127102359395"  />

**消息类型：**

position.x/y/z 与哪些方面相关

```
position.x/y/z = T(frame_id → child_frame_id).translation
```

**硬件部署:**

1. **安装**
   - 编码器通常安装在电机输出轴或轮子轴上，用于测量转角或转速。
   - 对于差速底盘：需要左右两个轮的编码器。
   - 对于麦克纳姆/全向轮：需要四个轮的编码器。
2. **数据接口**
   - 常见通过 **CAN、串口、I²C、SPI** 等上传。
   - 在机器人框架中通常会转成 `nav_msgs/Odometry` 或 `tf` 形式发布。

​		**作用：**

(1) 点云去畸变 (Motion Compensation)

机器人轮子在转动时，激光雷达扫描一帧点云会产生畸变。

- 编码器能提供位移量（Δx、Δy、Δθ）。
- 把这一运动补偿到点云中，消除运动引起的扭曲。

(2) 点云匹配的初始位姿

在做 **ICP / NDT** 等点云对齐时，用前后两帧编码器估计的位姿差作为 **初值**，大大减少收敛时间和错误匹配。

(3) 融合在 SLAM / 因子图优化中

在 **图优化 SLAM**（如 Cartographer, LIO-SAM 扩展版）中，编码器数据会被作为一个约束因子：

- 约束相邻帧的相对位姿变化
- 与雷达/IMU一起优化
- 提升系统对 **滑动、打滑** 情况的鲁棒性

## **imu**

IMU（惯性测量单元，包含加速度计和陀螺仪）通常用来**增强定位和感知效果**

**硬件部署：**

1. **安装位置**
   - IMU 通常固定在雷达传感器附近，最好在同一刚体上。
   - 目的是减少雷达与 IMU 的相对运动，避免因震动、安装偏移带来的误差。
   - 若安装在不同位置，需要通过标定获得 **外参（坐标系变换关系）**。
2. **时间同步**
   - 雷达点云和 IMU 数据要保证时间对齐。
   - 常见方式：
     - 硬件触发同步（雷达发触发信号给 IMU 或反过来）。
     - 软件时间戳同步（在驱动层统一打时间戳，精度略低）。
3. **外参标定**
   - 得到 IMU 坐标系与雷达坐标系的旋转 + 平移关系。
   - 可通过标定工具（如 Kalibr）实现。

**作用：**

1. **点云矫正**（去畸变）
2. **里程计预测**（作为点云匹配的先验）
3. **SLAM优化约束**（通过因子图融合 IMU 与雷达）

# ==使用方案==

## 2026使用

2026年雷达的使用主要是在由于底盘形状、梅林三维任务方面想要实现替代码盘，自动车上检测R1距离区分武器是否组装好以及进入梅林区后触发下一个区的任务，并在梅林区做导航，检测与R1的距离也可以。

- 关于检测r1：

  前提：障碍位置 **固定**、地图 **尺度明确（米**）

  实现：

  1. 通过固定坐标轴，划虚拟线的方式，通过激光跟踪，将r1当作障碍物在珊格地图中实时检测，或者实时点云处理，根据形状判断它的坐标（通过算法Euclidean Clustering（基于 kdtree））,处理前方 20 米，左右 ±8 米，高度 -1 ～ 2 米。
  2. 通过点云聚类，训练出r1模型，建图完毕后取出每一帧点云进行识别，就可以得知r1在所建地图下能扫到的任何位置的坐标，对比下来最符合赛事的是欧式聚类算法，也就是以欧几里距离为划分的简单方法；如果后续要与相机结合的话以图为数据结构类型的图聚类更好。可以用于识别物块，并且相对更加稳定

- 关于梅林区导航：

  **A*** 是搜索算法，可以使用曼哈顿距离作为边代价，图算法的代价计算。可视图算法修减邻域从而得到折线式的规划路线。

- **关于点云**

  - 点云稀疏/稠密：is_dense=false/true

    **激光雷达某些方向会没有返回值**

    这些情况下，点云数据**会被填 NaN 或被跳过**，导致 is_dense = false。

  - **z轴方向飘逸问题？**

    - 原因：
      1. 引入不完整维度上的约束（3DOF GNSS），可能导致部分维度（比如姿态）不连续。这种方法是大多数紧耦合LIO采用的思路。
      2. z轴方向分辨率低，观测信息少，点云精度差且不稳定，入射角过大
      3. 点云配准过程中的累积误差

    参考：https://blog.csdn.net/zhaoliang38/article/details/129054717

    - 解决：

      可以从地面特征点过滤等平面优化算法出发，可以从陀螺仪6个自由度约束出发，分析不同观测点约束强度，从而求解不同方向上的分量；位姿方面，存在qpep算法把旋转和平移的求解变成一个

      四元数多项式的闭式解，不需要迭代

    - fast lio的地图坐标系定义是采取第一帧雷达imu坐标系

     下一步准备先从算法参数上优化一下看看z轴的偏移量，再尝试45度倾斜安装雷达然后外参重标定

    论坛：https://www.zhihu.com/question/533056547

