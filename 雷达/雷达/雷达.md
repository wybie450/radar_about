```
export QT_ENABLE_HIGHDPI_SCALING=0
```

# ==雷达原理==

## cartographer算法

**子图构建（Submap）**：将扫描数据分批次保存到子图中，每个子图只覆盖环境的一部分。

**回环检测（Loop Closure）**：检测机器人回到之前走过的地方，修正累积误差。

**位姿图优化（Pose Graph Optimization）**：用图优化方法（比如 Ceres Solver）全局调整机器人的轨迹和地图，减少漂移。

- **用码盘（编码器）数据作为里程计（odometry）**
  1. **读取编码器数据**（脉冲数）
  2. **计算位移与速度**（根据轮半径、编码器分辨率、轮距）
  3. **发布里程计消息**（ROS 里就是 `/odom` 话题）

​		我感觉就是以码盘代替仿真中的差速控制器

![img](https://i-blog.csdnimg.cn/blog_migrate/b48e5cfc5f7cafe2d1e5aa6fe1f65750.png)

```
#include <chrono>
#include <cmath>
#include "rclcpp/rclcpp.hpp"
#include "nav_msgs/msg/odometry.hpp"
#include "geometry_msgs/msg/transform_stamped.hpp"
#include "tf2_ros/transform_broadcaster.h"

using namespace std::chrono_literals;

class EncoderOdomNode : public rclcpp::Node
{
public:
    EncoderOdomNode()
        : Node("encoder_odom_node"), x_(0.0), y_(0.0), theta_(0.0)
    {
        odom_pub_ = this->create_publisher<nav_msgs::msg::Odometry>("odom", 10);
        tf_broadcaster_ = std::make_shared<tf2_ros::TransformBroadcaster>(this);

        timer_ = this->create_wall_timer(20ms, std::bind(&EncoderOdomNode::update_odom, this));
    }

private:
    void update_odom()
    {
        // ====== 1. 读取编码器脉冲数 ======
        // 这里假设从硬件获取编码器数据（左、右轮累计脉冲数）
        int new_left_count = readLeftEncoder();
        int new_right_count = readRightEncoder();

        int delta_left = new_left_count - last_left_count_;
        int delta_right = new_right_count - last_right_count_;

        last_left_count_ = new_left_count;
        last_right_count_ = new_right_count;

        // ====== 2. 计算位移 ======
        double delta_s_left = 2 * M_PI * wheel_radius_ * (delta_left / static_cast<double>(encoder_ppr_));
        double delta_s_right = 2 * M_PI * wheel_radius_ * (delta_right / static_cast<double>(encoder_ppr_));

        double delta_s = (delta_s_left + delta_s_right) / 2.0;
        double delta_theta = (delta_s_right - delta_s_left) / wheel_base_;

        // 更新位置
        x_ += delta_s * cos(theta_ + delta_theta / 2.0);
        y_ += delta_s * sin(theta_ + delta_theta / 2.0);
        theta_ += delta_theta;

        // ====== 3. 发布里程计消息 ======
        auto now = this->get_clock()->now();

        // Odometry
        nav_msgs::msg::Odometry odom;
        odom.header.stamp = now;
        odom.header.frame_id = "odom";
        odom.child_frame_id = "base_link";
        odom.pose.pose.position.x = x_;
        odom.pose.pose.position.y = y_;
        odom.pose.pose.orientation.z = sin(theta_ / 2.0);
        odom.pose.pose.orientation.w = cos(theta_ / 2.0);

        odom_pub_->publish(odom);

        // TF
        geometry_msgs::msg::TransformStamped odom_tf;
        odom_tf.header.stamp = now;
        odom_tf.header.frame_id = "odom";
        odom_tf.child_frame_id = "base_link";
        odom_tf.transform.translation.x = x_;
        odom_tf.transform.translation.y = y_;
        odom_tf.transform.rotation = odom.pose.pose.orientation;

        tf_broadcaster_->sendTransform(odom_tf);
    }

    // 模拟读取硬件编码器
    int readLeftEncoder() { return left_encoder_count_; }
    int readRightEncoder() { return right_encoder_count_; }

    // 编码器硬件变量
    int left_encoder_count_ = 0;
    int right_encoder_count_ = 0;

    // 里程计参数
    double wheel_radius_ = 0.05;   // 轮半径 (m)
    double wheel_base_ = 0.3;      // 轮距 (m)
    int encoder_ppr_ = 1024;       // 每圈脉冲数

    // 上一次编码器计数
    int last_left_count_ = 0;
    int last_right_count_ = 0;

    // 机器人位姿
    double x_, y_, theta_;

    rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_pub_;
    std::shared_ptr<tf2_ros::TransformBroadcaster> tf_broadcaster_;
    rclcpp::TimerBase::SharedPtr timer_;
};

int main(int argc, char **argv)
{
    rclcpp::init(argc, argv);
    rclcpp::spin(std::make_shared<EncoderOdomNode>());
    rclcpp::shutdown();
    return 0;
}
```

## 高斯滤波

**高斯滤波是一种线性平滑滤波器，它使用高斯函数（正态分布）作为权重模板，对图像（或数据）中的每个像素（或点）进行加权平均，从而达到去噪、平滑的效果。**高斯滤波是一种基于高斯函数的图像或点云平滑技术，通过加权平均邻域数据来去除噪声，同时尽量保留边缘信息，是图像处理和计算机视觉中最常用的预处理方法之一。

## 卡尔曼滤波

**卡尔曼滤波（Kalman Filter, KF）是一种递归的、最优的、基于线性系统和高斯噪声假设的状态估计算法。**从不同角度殊途同归地得到了最优估计值和预测值、观测值的关系式，并且希望求解一个合适的K，使得最优估计值最接近真实值。通过观测值来修正预测值

有扩展卡尔曼等滤波方式，出现的原因是因为机器人由传感器计算代偿时考虑到的误差稳定，从而对其数据进行的优化。

## AMCL自适应蒙特卡洛定位

https://zhuanlan.zhihu.com/p/126199044

https://blog.csdn.net/p942005405/article/details/108569970

**AMCL是[Adaptive Monte Carlo Localization](https://zhida.zhihu.com/search?content_id=116174826&content_type=Article&match_order=1&q=Adaptive+Monte+Carlo+Localization&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NTU0MzY2OTAsInEiOiJBZGFwdGl2ZSBNb250ZSBDYXJsbyBMb2NhbGl6YXRpb24iLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoxMTYxNzQ4MjYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.48x82L3VaIaiByJWsZJ79UeZn-2f4T0mkOryR8bE_1I&zhida_source=entity)（也即是自适应蒙特卡洛定位）的简称，是基于多种蒙特卡洛融合算法在ROS/ROS2系统中的一种实现**。

蒙特卡洛：是一种思想或方法。举例：一个矩形里面有个不规则形状，怎么计算不规则形状的面积？不好算。但我们可以近似。拿一堆豆子，均匀的撒在矩形上，然后统计不规则形状里的豆子的个数和剩余地方的豆子个数。矩形面积知道的呀，所以就通过估计得到了不规则形状的面积。拿机器人定位来讲，它处在地图中的任何一个位置都有可能，这种情况我们怎么表达一个位置的置信度呢？我们也使用粒子，哪里的粒子多，就代表机器人在哪里的可能性高。

<img src="https://pic4.zhimg.com/v2-bd6a3609c2c30b3a20c7922f20a5e22d_r.jpg" alt="img" style="zoom:50%;" />



## 决策树

<img src="雷达.assets/image-20250814182039829.png" alt="image-20250814182039829" style="zoom: 33%;" />

<img src="雷达.assets/image-20250814194158011.png" alt="image-20250814194158011" style="zoom:33%;" />

## logistic回归

**Logistic 回归是一种用于解决二分类问题的统计学习方法。它通过一个 sigmoid 函数，将线性回归的输出映射到 [0, 1] 区间，从而表示某个样本属于某一类别的概率。**即在导航中遇到岔路时计算代价

<img src="雷达.assets/image-20250814194700437.png" alt="image-20250814194700437" style="zoom:33%;" />

- **为什么slam不能实时显示地图？**

  SLAM Toolbox 默认依赖 **base_link 相对于 odom 的运动** 来决定每帧 scan 的 pose。如果 odom 不动，SLAM Toolbox 会认为机器人没有移动：

  - scan 匹配不到新的位姿

  - 地图停留在第一帧

    **解决方法：**如果有 IMU 或 VIO/光学跟踪设备：

    - 每帧用实际位姿更新 x_, y_, th_
    - 发布 odom → base_link TF

  <img src="https://i-blog.csdnimg.cn/blog_migrate/3d54ed5dad0ac3117d6547d5a29beb6e.png" alt="在这里插入图片描述" style="zoom: 50%;" />

  

## PCL处理

**PCL（Point Cloud Library，点云库）** 是一个 **开源的、跨平台的 C++ 编程库**，专门用于 **2D/3D 点云数据的处理**。

它提供了大量高效、模块化的算法，支持：

- 点云的**读取 / 写入**
- **滤波（去噪、下采样）**
- **分割（地面提取、物体分离）**
- **特征提取（法线、曲率、关键点）**
- **配准（点云对齐，比如 ICP）**
- **表面重建**
- **聚类（比如物体分割）**
- **可视化**

## 图优化技术

相关文章(https://zhuanlan.zhihu.com/p/362600811)这个包含点对点的位姿矫正

​				(https://blog.csdn.net/datase/article/details/78473804)更易懂and第二篇难

**图优化（Graph Optimization）** 是一种 **把机器人的位姿和约束建模成图结构，并通过数学优化方法（如非线性最小二乘）来估计最优位姿和地图** 的技术。

<img src="https://pic2.zhimg.com/v2-24d9d58d788e8ecd3cf022b879d0d153_1440w.jpg" alt="img" style="zoom:50%;" />

| 作用                     | 说明                                                         | 应用场景                                |
| :----------------------- | :----------------------------------------------------------- | :-------------------------------------- |
| **全局位姿优化**         | 把机器人在不同时间点的位姿作为图的节点，把位姿之间的关系（比如运动约束、匹配约束）作为边，通过优化整张图，得到全局一致的、误差最小的位姿估计 | SLAM 后端优化、消除累计误差             |
| **消除累计误差（漂移）** | 单次定位（比如里程计、ICP）可能有小误差，随着时间推移会累积。图优化通过全局调整，减少这些误差，让轨迹更准 | 长时间运行的机器人导航与建图            |
| **闭环检测后的优化**     | 当检测到“我回到了之前的位置”，图优化可以基于这个信息，重新调整之前所有位姿，修正地图和轨迹 | SLAM 中的闭环校正                       |
| **融合多传感器约束**     | 可以把来自 ICP、里程计、IMU、视觉等不同传感器的约束都加入图中，统一优化 | 多传感器融合 SLAM（比如 LIO-SAM、VINS） |

## PCL技术

全名为**迭代最近点算法**，**是一种用于对齐两组点云数据的算法，它的目标是找到两个点云之间的** **最佳刚体变换（旋转 + 平移）**即优化位姿，使得这两组点云尽可能重合（对齐）。对齐方式有关键点检测等多种算法

| 作用                         | 说明                                                         | 应用场景                                                     |
| :--------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **点云对齐（配准）**         | 将不同时间、不同视角下的点云数据（比如激光雷达扫描）对齐到同一个坐标系下 | 激光 SLAM、三维重建、多帧点云拼接                            |
| **位姿估计（相对运动估计）** | 通过匹配当前点云和参考点云（如上一帧、地图），计算出机器人或传感器的 **相对位移和旋转（即位姿变化）** | 机器人定位、里程计替代、SLAM 中的 scan matching              |
| **辅助 SLAM 定位**           | 在没有 GPS、没有可靠里程计时，ICP 可以通过激光雷达扫描匹配来估计机器人当前的位置 | Laser-based SLAM（比如 Gmapping、Cartographer、slam_toolbox） |
| **闭环检测后的精确修正**     | 当 SLAM 系统发现可能回到了之前访问过的位置（闭环），用 ICP 对齐来精确计算位姿差异，从而修正累积误差 | SLAM 后端优化                                                |

-  **ICP 技术的作用是通过匹配点云数据，计算出机器人短时间内的精准位姿变换（比如移动了多少），主要用于局部定位与扫描匹配；而图优化技术的作用是把多个位姿和它们之间的关系构建成图，并通过全局优化方法消除累计误差，得到更准确、一致的轨迹和地图，主要用于 SLAM 的后端优化。**

## NDT技术

**NDT 是一种用于** **点云配准（Point Cloud Registration）** **的技术，它的核心思想是将点云数据划分为一个个网格（Voxel），并在每个网格内用一个概率分布（通常是多元高斯分布）来描述该区域内点云的形状，然后通过优化方法找到使两片点云最佳对齐的位姿（即旋转和平移）。**

- **源点云（Source）**：通常为当前时刻的激光雷达扫描数据
- **目标点云（Target）**：通常为上一帧点云、局部地图或者全局地图

## 建图技术

https://blog.csdn.net/qq_40695642/article/details/128472360

| 技术                              | 类型            | 传感器          | 是否需要里程计 | 主要特点                  | 适用场景             |
| :-------------------------------- | :-------------- | :-------------- | :------------- | :------------------------ | :------------------- |
| **gmapping**                      | 2D 激光 SLAM    | 2D 激光雷达     | 需要           | 经典粒子滤波，ROS常用     | 室内轮式机器人       |
| **hector_slam**                   | 2D/3D 激光 SLAM | 激光雷达        | 不需要         | 扫描匹配，不依赖里程计    | 无人机、无里程计平台 |
| **karto**                         | 激光 SLAM       | 激光雷达        | 通常需要       | 图优化，精度高            | 大范围建图           |
| **cartographer**                  | 2D/3D 激光 SLAM | 激光雷达        | 可无，推荐有   | Google出品，图优化强      | 大场景、高鲁棒性     |
| **rgbdslam**                      | RGB-D SLAM      | RGB-D相机       | 不需要         | 稠密3D建图，早期流行      | 小范围稠密地图       |
| **ORB-SLAM2 / 3**                 | 视觉SLAM        | 单目/双目/RGB-D | 不需要         | 特征点法，精度高，支持IMU | 视觉导航、AR/VR      |
| **loam / LeGO-LOAM**              | 3D激光SLAM      | 3D激光雷达      | 不需要         | 实时性强，特征匹配        | 无人车、无人机       |
| **三维重建（如Meshroom/COLMAP）** | SfM / MVS       | RGB图像         | 不需要         | 从照片中重建稠密3D模型    | 文物、建筑、VR内容   |

- **什么是占用栅格地图？**

  | 特点                 | 说明                                                         |
  | :------------------- | :----------------------------------------------------------- |
  | **2D 表示**          | 通常用于 **二维平面环境**（如室内、仓库、走廊等）            |
  | **概率性表示**       | 每个格子是“被占用”的概率，不是绝对的“有/无”                  |
  | **适合导航**         | 路径规划算法（如 A*、Dijkstra、DWA 等）通常直接使用这种地图  |
  | **由 SLAM 算法生成** | 很多 2D 激光 SLAM 算法（如 gmapping、cartographer）都会输出占用栅格地图 |
  | **可存储 / 可视化**  | 可保存为图片、PGM/PBM 文件，或 ROS 地图服务器中供导航使用    |

## 地图类型

| 地图类型             | 表示方式          | 维度    | 是否栅格化   | 是否依赖传感器   | 典型应用              |
| :------------------- | :---------------- | :------ | :----------- | :--------------- | :-------------------- |
| **1. 占用栅格地图**  | 概率栅格（0~100） | 2D      | 是           | 激光雷达         | 室内导航、ROS建图     |
| **2. 八叉树地图**    | 3D 栅格（概率）   | 3D      | 是（树结构） | 激光/RGB-D       | 无人机避障、3D导航    |
| **3. TSDF/体素地图** | 带符号距离场      | 3D      | 是（体素）   | RGB-D / 深度相机 | 表面重建、三维建模    |
| **4. 稠密点云地图**  | 3D点集合（x,y,z） | 3D      | 否           | 激光雷达 / RGB-D | 高保真建图、重建      |
| **5. 稀疏点云地图**  | 特征点/关键点     | 3D      | 否           | LiDAR / 视觉     | 定位、LiDAR SLAM      |
| **6. 特征点地图**    | 特征描述子 + 位置 | 2D / 3D | 否           | 相机             | 视觉SLAM、轻量定位    |
| **7. 拓扑地图**      | 节点 + 连接关系   | 2D / 3D | 否           | 抽象             | 高层导航、任务规划    |
| **8. 语义栅格地图**  | 栅格 + 语义标签   | 2D / 3D | 是           | 相机/激光+识别   | 智能导航、服务机器人  |
| **9. 代价地图**      | 栅格 + 代价值     | 2D      | 是           | 通常基于占用栅格 | 导航与路径规划（ROS） |



#### 鲁棒性

**鲁棒性（Robustness）** 是指一个系统、算法、模型或组件在面对**不确定性、错误输入、异常情况、外部干扰或部分失效**时，依然能够**保持稳定运行并维持预期性能**的能力

#### 代价计算

将普通坐标转换为栅格坐标后计算概率

**栅格坐标（Grid Coordinates / Cell Coordinates / Map Index）**

- **含义**：是 **二维数组索引**，表示某个点在 **代价地图（Costmap）或地图图像中对应的栅格（Cell）的位置**。
- **坐标系**：以 **栅格（格子）为单位**，每个格子代表地图上的一个固定大小的区域（比如 0.05m × 0.05m）。
- **表示方式**：通常是 `(mx, my)`，两个整数，分别表示 **行（y方向）和列（x方向）的索引号**。

导航我理解的目前原理就是规划器代价计算概率同时遍历所有点，就所有点生成一个最短路径，然后再把不用的点和未来点扔掉返回最近的需要点，通过pose发布给控制器，确认接收后在返回变化定下一个点(动作服务器)，然后控制器计算到目标点的步进和朝向逻辑最后返回线速度和角速度分装的twist消息，再有控制器内置的自动转换为话题的工具，发布为/cmd。主机可以通过订阅接收，或者直接定义全局变量取上坐标通过串口返回，所以可以自由选择要给主机传入的东西，而且他们通过.yaml参数启动，而不是launch的显式启动

# ==雷达分类==

## 2D雷达

### 轮趣科技雷达

### ——镭神M10P

windows上位机看点云图

**串口通信**

1.  轮趣的镭神雷达里面有一个串口转接模块，就是在雷达盒子里的那个比较小的那个转接模块

2.  把雷达的线插进去

    像这样：

3.  随便找一个比较粗的usb->type-c线（比如手机充电线）一边连电脑自带的usb（必须是自带的，别连在拓展坞上，主要是供电不足），一边连串口转接模块

**网口通信（以太网通信）**

步骤 1：进入你的 ROS 2 工作空间

```
cd ~/ros2_ws/src
```

步骤 2：克隆 leishen_driver 仓库

**GitHub 镜像地址**（常用，适配 ROS 2）：

```
git clone https://github.com/LeishenLidar/Leishen_ROS2_Driver.git
```

或国内镜像（如访问较慢）：

```
git clone https://gitee.com/leishen_lidar/Leishen_ROS2_Driver.git
```

步骤 3：编译驱动包

返回工作空间根目录并编译：

```
cd ~/ros2_ws
colcon build --packages-select leishen_lidar_sdk
```

> 注意：有时驱动包名叫 `leishen_lidar_sdk`，也有叫 `lslidar_driver` 的旧版本，取决于你克隆的仓库。

### 锐驰智光雷达

### ——LakiBeam 1

-   官网先奉上：https://www.richbeam.com/download

windows上位机看点云图：

-   官网先奉上：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2025-06-05/cck4ydmasmbtvfe0q7wpiaw60w93hcqn_.pdf
-   B站视频奉上：【锐驰智光LakiBeam单线系列激光雷达操作教程（Windows版）】https://www.bilibili.com/video/BV1LN4y1w7WV?vd_source=a149de35c7ad820661d6f346b01b0eb1
-   自带rg45网口和dc电源接口

**网口通信**（以太网通信）

1.  网口通信也是一样的供电支持9到36V，还是和镭神一样。直接用队里的充电宝或者12V的电池供电
2.  雷达的接口是DC55-21供电接口和一个RJ45以太网口，使用雷达的时候一个供电，一个通过网线连到主机或电脑上就行

----------------------------------------------------------------------------------------------------------------------------

1.  雷达的串口仅支持5V供电，轮趣的镭神雷达也是5V供电
2.  然后看type-c接口的位置：把两个梅花六角螺丝拧下来里面就是type-c接口（我没试过水，谨慎使用）

**linux上用ROS2跑雷达**

-   官网先奉上：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2024-05-23/q26paa5fm40da9cbjp2roz4ihgs2vyf6_.pdf

1. ###### 网络IP设置

   -   查找网络接口`ip a`，查找雷达虚拟网卡，如`enxaaa8d18544d7`

   -   启动虚拟网卡`sudo ip link set enxaaa8d18544d7 up`

   - 进入http://192.168.8.2/config.html，设置dhcp动态分配地址`192.168.8.1`

     - 这里如果是usb-typec就这么设置,pcl以太网的话要设为`192.168.198.1`

   - 运行`ping 192.168.8.2`，显示ping通即可

     - 同理，rg45这里ping`192.168.198.2`

   -   但是这样做会有一个缺点，就是每次重启都得设置一次，有点麻烦，可以这样：在终端里运行

       ```python
       sudo nano /etc/netplan/01-network-manager-all.yaml
       ```

       然后Ctrl+o进入写入模式，修改为以下内容：

       ```python
       network:
         version: 2
         renderer: NetworkManager
         ethernets:
           enp57s0:
             addresses: [192.168.198.1/24]
             dhcp4: no
             optional: true
       ```

       运行`sudo network apply`应用

4. ###### 跑代码

   -   点进去，下载驱动（也可以说是代码）：https://636d-cms-6ghze6jgb6d413ae-1317651364.tcb.qcloud.la/cloudbase-cms/upload/2025-04-14/dkup0izne6ocyz49x2bmvv61ehpobgdq_.zip

   -   创建一个文件夹（用来放驱动），在文件夹里面创建src文件夹，把下载好的驱动提取到src文件夹里，在src的上级目录下打开终端，运行`colcon build`构建驱动。应该会构建8到30秒

   -   然后就可以运行驱动了：

       更新路径：

       ```python
       source ./install/setup.bash
       ```

       运行驱动：

       ```python
       ros2 launch lakibeam1 lakibeam1_scan.launch.py
       ```

       **然后这里注意了：**运行驱动后，终端里面会不断地显示数据，这才是正常现象，如果卡在一个地方，说明出问题了

       然后可能出的问题有两个：

       -   防火墙
       
       -   以太网口有多个IP（导致驱动检测到的数据不全是雷达的，然后卡在一个地方一直卡着）

           解决方法：只抓取和雷达相关的信息：

           ```python
           sudo tcpdump -i eth0 -n host 192.168.198.2
           ```
       
           没安装先安装：
       
           ```python
           sudo apt update
           sudo apt install tcpdump
           ```
       
       
       - 驱动中的remote.cpp作用：
       
         通过 HTTP PUT 请求，向传感器设备（如 LiDAR）发送配置参数，比如设置扫描频率、滤波参数等，通过多个 HTTP GET 请求，获取传感器设备的各种状态信息，比如型号、版本、运行负载、电机转速、扫描范围等，并通过 ROS2 打印日志，通信方式是使用 **libcurl** 发送 HTTP 请求，与传感器设备进行 **RESTful API 通信**
       
         ###### 什么是libcurl,cURL,curl?
       
         <img src="https://i-blog.csdnimg.cn/blog_migrate/dfeb4a7f8724702ae3630d45d38ea60e.png" alt="在这里插入图片描述" style="zoom:50%;" />
       
       新建终端，然后更新路径：
       
       ```python
       source ./install/setup.bash
       ```
       
       查看点云图（在Rviz里面）
       
       ```python
       ros2 launch lakibeam1 lakibeam1_scan_view.launch.py
       ```
   
   **总结一下：**设置ip这个操作主要是为了设备上收集到数据后可以通过主机的scan话题发布，但通过不同线缆连接后以太网协议是不一样的，网卡方面雷达自带的虚拟网卡就很方便不用安插更多的东西

# ==建图方法==

## **gmapping**



<img src="https://pica.zhimg.com/v2-74708d344109219df04fd421ec0f3310_1440w.jpg" alt="img" style="zoom:50%;" />

## **cartigrapher**

从原理中我们得知，它比起其他算法带有回环检测功能，并且不一定需要imu即odem反馈数据，很适合测试雷达时手持建图，解决了没有里程计卡在第一帧的问题

- 在官驱config下创建.iua文件

  ```
  include "map_builder.lua"
  include "trajectory_builder.lua"
  options = {
    map_builder = MAP_BUILDER,
    trajectory_builder = TRAJECTORY_BUILDER,
    map_frame = "map",
    --  跟踪和发布的frame都改成雷达的frameID
    tracking_frame = "laser",
    published_frame = "laser",
    odom_frame = "odom",
    -- true改为false，不用提供里程计数据
    provide_odom_frame = false,
    -- false改为true，仅发布2D位资
    publish_frame_projected_to_2d = false,
    use_pose_extrapolator = true,
    -- true改为false，不使用里程计数据
    use_odometry = false,
    use_nav_sat = false,
    use_landmarks = false,
    -- 0改为1,使用一个雷达
    num_laser_scans = 1,
    -- 1改为0，不使用多波雷达
    num_multi_echo_laser_scans = 0,
    -- 10改为1，1/1=1等于不分割
    num_subdivisions_per_laser_scan = 1,
    num_point_clouds = 0,
    lookup_transform_timeout_sec = 0.2,
    submap_publish_period_sec = 0.3,
    pose_publish_period_sec = 5e-3,
    trajectory_publish_period_sec = 30e-3,
    rangefinder_sampling_ratio = 1.,
    odometry_sampling_ratio = 1.,
    fixed_frame_pose_sampling_ratio = 1.,
    imu_sampling_ratio = 1.,
    landmarks_sampling_ratio = 1.,
  }
  -- false改为true，启动2D SLAM
  MAP_BUILDER.use_trajectory_builder_2d = true
  -- 0改成0.10,比机器人半径小的都忽略
  TRAJECTORY_BUILDER_2D.min_range = 0.10
  -- 30改成3.5,限制在雷达最大扫描范围内，越小一般越精确些
  TRAJECTORY_BUILDER_2D.max_range = 5.5
  -- 5改成3,传感器数据超出有效范围最大值
  TRAJECTORY_BUILDER_2D.missing_data_ray_length = 3.
  -- true改成false,不使用IMU数据，大家可以开启，然后对比下效果
  TRAJECTORY_BUILDER_2D.use_imu_data = false
  -- false改成true,使用实时回环检测来进行前端的扫描匹配
  TRAJECTORY_BUILDER_2D.use_online_correlative_scan_matching = true 
  -- 1.0改成0.1,提高对运动的敏感度
  -- TRAJECTORY_BUILDER_2D.motion_filter.max_angle_radians = math.rad(0.1)
  -- 0.55改成0.65,Fast csm的最低分数，高于此分数才进行优化。
  POSE_GRAPH.constraint_builder.min_score = 0.65
  --0.6改成0.7,全局定位最小分数，低于此分数则认为目前全局定位不准确
  POSE_GRAPH.constraint_builder.global_localization_min_score = 0.7
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.linear_search_window = 0.1
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.translation_delta_cost_weight = 10.
  TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.rotation_delta_cost_weight = 1e-1
  POSE_GRAPH.optimization_problem.huber_scale = 1e2
  POSE_GRAPH.optimize_every_n_nodes = 35
  -- 设置0可关闭全局SLAM
  -- POSE_GRAPH.optimize_every_n_nodes = 0
  return options
  ```

- 运用官方库编写launch

  ```
  from launch import LaunchDescription
  from launch_ros.actions import Node
  
  def generate_launch_description():
      return LaunchDescription([
          Node(
              package='cartographer_ros',
              executable='cartographer_node',
              name='cartographer_node',
              output='screen',
              parameters=[{
                  'use_sim_time': False
              }],
              arguments=[
                  '-configuration_directory', '/home/wybie/lakibeam1_ENGINE/src/Lakibeam_ROS2_Driver-main/config',
                  '-configuration_basename', 'backpack_2d.lua'
              ],
              remappings=[('/scan', '/scan')]
          ),
          Node(
              package='cartographer_ros',
              executable='cartographer_occupancy_grid_node',
              parameters=[{'resolution': 0.05}],
              remappings=[('/map', '/map')]
          )
      ])
  
  ```

  - `use_sim_time=false`需要实时建图不需要仿真时间时，这个要关掉

以上，相比原理来说实现相对简单得多，lua文件有开源的格式和讲解，launch中也只需要调用算法库文件的节点就能解决

## slam-toolbox

**运行命令**

```
ros2 launch slam_toolbox online_async_launch.py use_sim_time:=True \
  slam_params_file:=/home/wybie/lakibeam1_ENGINE/src/Lakibeam_ROS2_Driver-main/config/slam_config.yaml \
  log_level:=debug
```

**slam_config.yaml**

```
slam_toolbox:
  ros__parameters:
    use_sim_time: false        # 如果不在仿真或 rosbag
    slam_mode: true            # 运行 SLAM
    resolution: 0.05           # 地图分辨率，单位 m
    map_update_interval: 1.0   # 地图更新周期，单位 s
    max_laser_range: 20.0      # 雷达最大检测距离

    # 坐标系
    odom_frame: "odom"          # 没有里程计时，暂时让 odom = map
    map_frame: "map"
    base_frame: "base_link"    # 上面 static TF 发布的 base_link
    scan_topic: "/scan"        # 雷达话题

    # 发布 TF
    publish_tf: true
    map_publish_period: 1.0

    # 其它参数
    minimum_time_interval: 0.5
    transform_publish_period: 0.05
    enable_interactive_mode: false
    use_scan_matching: true     # 没有里程计时必须打开
```



# ==扫盲==

## 北极熊

- **导航开源**

  https://bbs.robomaster.com/article/714072?source=1                                                                                                                pid控制器流程：

  1.从 global_plan 中裁剪出 local_costmap 范围内的路径

  2.根据当前速度自适应计算 carrot_pose，即前瞻点

  3.解耦线速度和角速度的计算

  使用平移 PID 和旋转 PID，根据期望值分别输出线速度和角速度，对于 RM 场景可选择仅输出线速度。

## 决策树

概述：https://zhuanlan.zhihu.com/p/133838427

原理讲解：https://blog.csdn.net/GreenYang5277/article/details/104500739

都没看懂啊干

## 双主机ROS

ros2是分布式通信，通过交换机连接到同一个局域网，然后固定id号，避免话题冲突

# ==传感器与里程计部署==

## **里程计**

有对应的话题与消息类型，可在启动建图的文件中调用，主要目的是做到识别位姿和里程记录，做到导航的实时性

**硬件部署:**

1. **安装**
   - 编码器通常安装在电机输出轴或轮子轴上，用于测量转角或转速。
   - 对于差速底盘：需要左右两个轮的编码器。
   - 对于麦克纳姆/全向轮：需要四个轮的编码器。
2. **数据接口**
   - 常见通过 **CAN、串口、I²C、SPI** 等上传。
   - 在机器人框架中通常会转成 `nav_msgs/Odometry` 或 `tf` 形式发布。

​		**作用：**

(1) 点云去畸变 (Motion Compensation)

机器人轮子在转动时，激光雷达扫描一帧点云会产生畸变。

- 编码器能提供位移量（Δx、Δy、Δθ）。
- 把这一运动补偿到点云中，消除运动引起的扭曲。

(2) 点云匹配的初始位姿

在做 **ICP / NDT** 等点云对齐时，用前后两帧编码器估计的位姿差作为 **初值**，大大减少收敛时间和错误匹配。

(3) 融合在 SLAM / 因子图优化中

在 **图优化 SLAM**（如 Cartographer, LIO-SAM 扩展版）中，编码器数据会被作为一个约束因子：

- 约束相邻帧的相对位姿变化
- 与雷达/IMU一起优化
- 提升系统对 **滑动、打滑** 情况的鲁棒性

**代码示例：**

```
#include <iostream>
#include <cmath>
#include <Eigen/Dense>

struct EncoderData {
    double left_ticks;
    double right_ticks;
    double resolution;   // 每圈脉冲数
    double wheel_radius; // 轮半径 (m)
    double wheel_base;   // 轮距 (m)
};

struct Pose2D {
    double x, y, theta;
};

/// 根据编码器计算位姿增量
Pose2D computeOdometry(const EncoderData& enc, double last_left, double last_right) {
    double d_left  = (enc.left_ticks  - last_left)  * (2 * M_PI * enc.wheel_radius / enc.resolution);
    double d_right = (enc.right_ticks - last_right) * (2 * M_PI * enc.wheel_radius / enc.resolution);

    double d_center = (d_left + d_right) / 2.0;
    double d_theta  = (d_right - d_left) / enc.wheel_base;

    return {d_center * cos(d_theta/2), d_center * sin(d_theta/2), d_theta};
}

/// 将编码器位姿增量作为雷达点云对齐的初值
Eigen::Matrix4d predictPose(const Pose2D& last_pose, const Pose2D& delta) {
    Pose2D new_pose;
    new_pose.x     = last_pose.x + delta.x * cos(last_pose.theta) - delta.y * sin(last_pose.theta);
    new_pose.y     = last_pose.y + delta.x * sin(last_pose.theta) + delta.y * cos(last_pose.theta);
    new_pose.theta = last_pose.theta + delta.theta;

    Eigen::Matrix4d T = Eigen::Matrix4d::Identity();
    T(0,0) = cos(new_pose.theta); T(0,1) = -sin(new_pose.theta); T(0,3) = new_pose.x;
    T(1,0) = sin(new_pose.theta); T(1,1) =  cos(new_pose.theta); T(1,3) = new_pose.y;
    return T;
}

int main() {
    EncoderData enc{1000, 1000, 2048, 0.05, 0.3}; // 例子参数
    Pose2D last_pose{0, 0, 0};

    // 计算增量
    Pose2D delta = computeOdometry(enc, 0, 0);

    // 融合为预测位姿
    Eigen::Matrix4d T_pred = predictPose(last_pose, delta);

    std::cout << "预测位姿矩阵: \n" << T_pred << std::endl;

    // 在这里可以把 T_pred 作为点云匹配 (ICP/NDT) 的初始值
    return 0;
}
```



## **imu**

IMU（惯性测量单元，包含加速度计和陀螺仪）通常用来**增强定位和感知效果**

**硬件部署：**

1. **安装位置**
   - IMU 通常固定在雷达传感器附近，最好在同一刚体上。
   - 目的是减少雷达与 IMU 的相对运动，避免因震动、安装偏移带来的误差。
   - 若安装在不同位置，需要通过标定获得 **外参（坐标系变换关系）**。
2. **时间同步**
   - 雷达点云和 IMU 数据要保证时间对齐。
   - 常见方式：
     - 硬件触发同步（雷达发触发信号给 IMU 或反过来）。
     - 软件时间戳同步（在驱动层统一打时间戳，精度略低）。
3. **外参标定**
   - 得到 IMU 坐标系与雷达坐标系的旋转 + 平移关系。
   - 可通过标定工具（如 Kalibr）实现。

**作用：**

1. **点云矫正**（去畸变）
2. **里程计预测**（作为点云匹配的先验）
3. **SLAM优化约束**（通过因子图融合 IMU 与雷达）

**代码示例：**

imu.cpp

```
#include <rclcpp/rclcpp.hpp>
#include <sensor_msgs/msg/imu.hpp>
#include <sensor_msgs/msg/point_cloud2.hpp>
#include <nav_msgs/msg/odometry.hpp>

#include <pcl/point_cloud.h>
#include <pcl/point_types.h>
#include <pcl/registration/icp.h>
#include <pcl_conversions/pcl_conversions.h>

#include <Eigen/Dense>
#include <deque>
#include <optional>
#include <unordered_map>

using std::placeholders::_1;
using rclcpp::Time;

struct ImuSample {
  double t;                // sec (ROS time)
  Eigen::Vector3d gyro;    // rad/s
  Eigen::Vector3d accel;   // m/s^2
};

struct Pose {
  Eigen::Quaterniond q = Eigen::Quaterniond::Identity();
  Eigen::Vector3d p = Eigen::Vector3d::Zero();
  Eigen::Vector3d v = Eigen::Vector3d::Zero();
};

static inline Eigen::Matrix3d Skew(const Eigen::Vector3d& w){
  Eigen::Matrix3d W;
  W <<     0, -w.z(),  w.y(),
        w.z(),     0, -w.x(),
       -w.y(),  w.x(),     0;
  return W;
}

// 小角度指数映射
static inline Eigen::Quaterniond ExpSO3(const Eigen::Vector3d& wdt){
  double th = wdt.norm();
  if (th < 1e-8) return Eigen::Quaterniond(1, 0, 0, 0);
  Eigen::Vector3d a = wdt / th;
  double s = sin(th/2.0);
  return Eigen::Quaterniond(cos(th/2.0), a.x()*s, a.y()*s, a.z()*s);
}

// 时间索引辅助：二分查找
template<typename T>
int lower_bound_time(const std::deque<T>& buf, double t){
  int l=0, r=(int)buf.size();
  while(l<r){
    int m=(l+r)/2;
    if(buf[m].t < t) l=m+1; else r=m;
  }
  return l; // 第一个 >= t 的下标
}

class ImuPreintegrator {
public:
  void push(const ImuSample& s){ buf_.push_back(s); prune(); }

  // 获取 [t0, t1] 的相对位姿（预积分），忽略偏置，简单重力补偿
  // 返回 delta_R, delta_p, delta_v
  std::optional<std::tuple<Eigen::Quaterniond,Eigen::Vector3d,Eigen::Vector3d>>
  delta(double t0, double t1){
    if(buf_.size() < 2 || t1 <= t0) return std::nullopt;
    // 保证覆盖区间
    if(buf_.front().t > t0 || buf_.back().t < t1) return std::nullopt;

    Eigen::Quaterniond dR = Eigen::Quaterniond::Identity();
    Eigen::Vector3d dv = Eigen::Vector3d::Zero();
    Eigen::Vector3d dp = Eigen::Vector3d::Zero();

    // 简化：分段欧拉积分
    // 取出落在区间内的采样
    int i = lower_bound_time(buf_, t0);
    if(i==0) i=1;
    double last_t = t0;
    Eigen::Vector3d last_gyro = buf_[i-1].gyro;
    Eigen::Vector3d last_acc  = buf_[i-1].accel;

    // 重力方向（世界系）
    const Eigen::Vector3d g(0,0,-9.81);

    auto step = [&](double dt, const Eigen::Vector3d& gyro, const Eigen::Vector3d& acc){
      // 旋转积分
      Eigen::Quaterniond dRstep = ExpSO3(gyro * dt);
      dR = dR * dRstep;

      // 加速度转到世界系并去重力
      Eigen::Vector3d aw = dR * acc + g; // 注意：这里 dR 是从body到世界的近似累计
      // 速度、位置积分
      dv += aw * dt;
      dp += dv * dt + 0.5 * aw * dt * dt;
    };

    // 遍历 IMU 片段
    for(; i < (int)buf_.size(); ++i){
      double t = std::min(buf_[i].t, t1);
      double dt = t - last_t;
      if(dt > 0){
        // 线性插值陀螺与加计
        Eigen::Vector3d gyro = last_gyro + (buf_[i].gyro - last_gyro) * ((t - last_t)/ (buf_[i].t - last_t + 1e-9));
        Eigen::Vector3d acc  = last_acc  + (buf_[i].accel - last_acc ) * ((t - last_t)/ (buf_[i].t - last_t + 1e-9));
        step(dt, gyro, acc);
        last_t = t;
      }
      if(buf_[i].t >= t1) break;
      last_gyro = buf_[i].gyro;
      last_acc  = buf_[i].accel;
    }
    return std::make_optional(std::make_tuple(dR, dp, dv));
  }

  // 取时刻 t 的姿态（相对最早样本），仅用于去畸变插值（忽略平移）
  std::optional<Eigen::Quaterniond> orientation_at(double t){
    if(buf_.size()<2 || t < buf_.front().t || t > buf_.back().t) return std::nullopt;
    auto od = delta(buf_.front().t, t);
    if(!od) return std::nullopt;
    return std::get<0>(*od);
  }

  // 保持最近一段时间的数据（比如 5s）
  void set_window(double seconds){ window_ = seconds; }

private:
  void prune(){
    double tmax = buf_.empty() ? 0 : buf_.back().t;
    while(!buf_.empty() && (tmax - buf_.front().t) > window_) buf_.pop_front();
  }
  std::deque<ImuSample> buf_;
  double window_ = 5.0;
};

class LioMinimalNode : public rclcpp::Node {
public:
  LioMinimalNode() : Node("lio_minimal"){
    using std::chrono::milliseconds;
    imu_sub_ = create_subscription<sensor_msgs::msg::Imu>(
      "/imu", 2000, std::bind(&LioMinimalNode::imu_cb, this, _1));
    cloud_sub_ = create_subscription<sensor_msgs::msg::PointCloud2>(
      "/points_raw", 10, std::bind(&LioMinimalNode::cloud_cb, this, _1));

    odom_pub_ = create_publisher<nav_msgs::msg::Odometry>("/lio/odom", 20);
    cloud_pub_ = create_publisher<sensor_msgs::msg::PointCloud2>("/lio/deskewed", 5);

    preint_.set_window(10.0);
    last_pose_.q.setIdentity();
    last_pose_.p.setZero();

    RCLCPP_INFO(get_logger(), "LIO minimal node started.");
  }

private:
  // ---------------- IMU 回调：入队 ----------------
  void imu_cb(const sensor_msgs::msg::Imu::SharedPtr msg){
    double t = stamp_to_sec(msg->header.stamp);
    ImuSample s;
    s.t = t;
    s.gyro = Eigen::Vector3d(msg->angular_velocity.x,
                             msg->angular_velocity.y,
                             msg->angular_velocity.z);
    s.accel = Eigen::Vector3d(msg->linear_acceleration.x,
                              msg->linear_acceleration.y,
                              msg->linear_acceleration.z);
    preint_.push(s);
  }

  // ---------------- 点云回调：去畸变 + ICP ----------------
  void cloud_cb(const sensor_msgs::msg::PointCloud2::SharedPtr msg){
    if(preint_empty()) {
      RCLCPP_WARN_THROTTLE(get_logger(), *get_clock(), 2000, "No IMU yet, skip cloud.");
      return;
    }

    // 1) 提取每点时间（若没有time字段，退化为整帧同一时间）
    std::vector<Eigen::Vector4f> pts; pts.reserve(msg->width * msg->height);
    std::vector<double> rel_time; rel_time.reserve(pts.capacity());
    bool has_time = has_field(*msg, "time") || has_field(*msg, "t");

    // 读取 x,y,z,(time)
    sensor_msgs::PointCloud2ConstIterator<float> it_x(*msg, "x");
    sensor_msgs::PointCloud2ConstIterator<float> it_y(*msg, "y");
    sensor_msgs::PointCloud2ConstIterator<float> it_z(*msg, "z");

    // 可选时间字段
    std::optional<sensor_msgs::PointCloud2ConstIterator<float>> it_time;
    if (has_field(*msg, "time")) {
      it_time.emplace(sensor_msgs::PointCloud2ConstIterator<float>(*msg, "time"));
    } else if (has_field(*msg, "t")) {
      it_time.emplace(sensor_msgs::PointCloud2ConstIterator<float>(*msg, "t"));
    }

    size_t N = msg->width * msg->height;
    pts.resize(N);
    rel_time.resize(N, 0.0);

    for(size_t i=0; i<N; ++i, ++it_x, ++it_y, ++it_z){
      pts[i] = Eigen::Vector4f(*it_x, *it_y, *it_z, 1.0f);
      if (it_time.has_value()){
        rel_time[i] = (double)(**it_time); ++(*it_time);
      }else{
        rel_time[i] = 0.0; // 无时间字段 -> 不做去畸变
      }
    }

    double t_end = stamp_to_sec(msg->header.stamp);
    double t_start = t_end - estimate_scan_duration(rel_time);

    // 2) 用 IMU 为每个点做去畸变（仅旋转）
    // 计算参考姿态：帧起始时刻
    auto q0_opt = preint_.orientation_at(t_start);
    auto q1_opt = preint_.orientation_at(t_end);
    if(!q0_opt || !q1_opt){
      RCLCPP_WARN(get_logger(), "IMU buffer does not cover scan interval, skip.");
      return;
    }
    Eigen::Quaterniond q0 = *q0_opt;

    pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_deskew(new pcl::PointCloud<pcl::PointXYZ>());
    cloud_deskew->reserve(N);

    for(size_t i=0; i<N; ++i){
      double ti = t_start + rel_time[i];
      Eigen::Quaterniond qi = q0;
      if(it_time.has_value()){
        if(auto qo = preint_.orientation_at(ti)) qi = *qo; // 插值姿态
      }
      // 把点从采样时刻旋转到扫描起始时刻（反向旋转）
      Eigen::Quaterniond dq = q0 * qi.conjugate();
      Eigen::Vector3d p = dq * Eigen::Vector3d(pts[i].x(), pts[i].y(), pts[i].z());
      cloud_deskew->push_back(pcl::PointXYZ(p.x(), p.y(), p.z()));
    }

    // 3) 用 IMU 预积分做初始位姿猜测，再做 ICP
    Eigen::Matrix4f init = Eigen::Matrix4f::Identity();
    if(last_cloud_){
      if(auto d = preint_.delta(t_last_scan_end_, t_end)){
        Eigen::Quaterniond dR = std::get<0>(*d);
        Eigen::Vector3d dp    = std::get<1>(*d);
        Eigen::Matrix3d R = dR.toRotationMatrix();
        init.block<3,3>(0,0) = R.cast<float>();
        init.block<3,1>(0,3) = dp.cast<float>();
      }
      pcl::IterativeClosestPoint<pcl::PointXYZ, pcl::PointXYZ> icp;
      icp.setMaxCorrespondenceDistance(1.0);
      icp.setMaximumIterations(30);
      icp.setTransformationEpsilon(1e-6);
      icp.setEuclideanFitnessEpsilon(1e-6);
      icp.setInputSource(cloud_deskew);
      icp.setInputTarget(last_cloud_);
      pcl::PointCloud<pcl::PointXYZ> aligned;
      icp.align(aligned, init);

      Eigen::Matrix4f T = icp.hasConverged() ? icp.getFinalTransformation() : init;

      // 累计到世界系（这里简化为把上一帧位姿当作世界系基准的连续累计）
      Eigen::Matrix4d Td = last_Twb_ * T.cast<double>();
      last_Twb_ = Td;

      // 发布 Odom
      publish_odom(Td, msg->header.stamp);

    }else{
      last_Twb_.setIdentity();
      publish_odom(last_Twb_, msg->header.stamp);
    }

    // 4) 发布去畸变点云
    sensor_msgs::msg::PointCloud2 out_msg;
    pcl::toROSMsg(*cloud_deskew, out_msg);
    out_msg.header = msg->header;
    out_msg.header.frame_id = "lidar"; // 视需要修改
    cloud_pub_->publish(out_msg);

    // 更新缓存
    last_cloud_.reset(new pcl::PointCloud<pcl::PointXYZ>(*cloud_deskew));
    t_last_scan_end_ = t_end;
  }

  // ---------------- 工具函数 ----------------
  bool preint_empty() const { return true_if_.empty && false; } // 占位避免误用
  static double stamp_to_sec(const rclcpp::Time& t){
    return (double)t.sec + 1e-9 * (double)t.nanosec;
  }

  static bool has_field(const sensor_msgs::msg::PointCloud2& cloud, const std::string& name){
    for(const auto& f : cloud.fields) if(f.name == name) return true;
    return false;
  }

  // 估计扫描时长：如果有每点时间字段，用最大值；否则假设 0
  static double estimate_scan_duration(const std::vector<double>& rel_time){
    if(rel_time.empty()) return 0.0;
    double mx = 0.0;
    for(double t : rel_time) if(t > mx) mx = t;
    return mx;
  }

  void publish_odom(const Eigen::Matrix4d& Twb, const rclcpp::Time& stamp){
    Eigen::Matrix3d R = Twb.block<3,3>(0,0);
    Eigen::Vector3d p = Twb.block<3,1>(0,3);
    Eigen::Quaterniond q(R);

    nav_msgs::msg::Odometry od;
    od.header.stamp = stamp;
    od.header.frame_id = "map";     // 世界系
    od.child_frame_id = "base_link";
    od.pose.pose.position.x = p.x();
    od.pose.pose.position.y = p.y();
    od.pose.pose.position.z = p.z();
    od.pose.pose.orientation.w = q.w();
    od.pose.pose.orientation.x = q.x();
    od.pose.pose.orientation.y = q.y();
    od.pose.pose.orientation.z = q.z();
    odom_pub_->publish(od);
  }

  // ---------------- 成员 ----------------
  rclcpp::Subscription<sensor_msgs::msg::Imu>::SharedPtr imu_sub_;
  rclcpp::Subscription<sensor_msgs::msg::PointCloud2>::SharedPtr cloud_sub_;
  rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_pub_;
  rclcpp::Publisher<sensor_msgs::msg::PointCloud2>::SharedPtr cloud_pub_;
  ImuPreintegrator preint_;

  pcl::PointCloud<pcl::PointXYZ>::Ptr last_cloud_;
  Eigen::Matrix4d last_Twb_ = Eigen::Matrix4d::Identity();
  double t_last_scan_end_ = 0.0;

  // 防止误用的占位结构
  struct { bool empty = false; } true_if_;
};

int main(int argc, char** argv){
  rclcpp::init(argc, argv);
  rclcpp::spin(std::make_shared<LioMinimalNode>());
  rclcpp::shutdown();
  return 0;
}
```

cmakelists.txt

```
cmake_minimum_required(VERSION 3.10)
project(lio_minimal)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_BUILD_TYPE Release)

find_package(ament_cmake REQUIRED)
find_package(rclcpp REQUIRED)
find_package(sensor_msgs REQUIRED)
find_package(nav_msgs REQUIRED)
find_package(PCL REQUIRED COMPONENTS common io filters registration)
find_package(pcl_conversions REQUIRED)
find_package(Eigen3 REQUIRED)

add_executable(lio_minimal_node src/lio_minimal.cpp)
ament_target_dependencies(lio_minimal_node
  rclcpp sensor_msgs nav_msgs pcl_conversions)

target_include_directories(lio_minimal_node PRIVATE
  ${EIGEN3_INCLUDE_DIR} ${PCL_INCLUDE_DIRS})
target_link_libraries(lio_minimal_node ${PCL_LIBRARIES})

install(TARGETS lio_minimal_node
  DESTINATION lib/${PROJECT_NAME})

ament_package()
```

package.xml

```
<?xml version="1.0"?>
<package format="3">
  <name>lio_minimal</name>
  <version>0.0.1</version>
  <description>Minimal Lidar+IMU (de-skew + preintegration + ICP) example</description>
  <maintainer email="you@example.com">you</maintainer>
  <license>MIT</license>

  <buildtool_depend>ament_cmake</buildtool_depend>

  <depend>rclcpp</depend>
  <depend>sensor_msgs</depend>
  <depend>nav_msgs</depend>
  <depend>pcl_conversions</depend>

  <exec_depend>pcl_ros</exec_depend>
</package>
```





